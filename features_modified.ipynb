{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Prominence Detection\n",
    "Speech Prominence Detection is the process of identifying the most important or prominent parts of speech in an audio signal. Prominence refers to the degree of emphasis or attention that a particular word or phrase receives in spoken language, which is often conveyed through variations in pitch, loudness, and timing. Speech Prominence Detection is an essential task in speech processing and natural language understanding, with a wide range of applications including speech recognition, sentiment analysis, and language translation. The goal of this project is to develop a machine learning model that can accurately identify the prominent parts of speech in a given audio signal. This project will involve feature extraction, model training, and evaluation, with the aim of achieving high accuracy and generalizability on a diverse range of speech datasets. The results of this project could have significant implications for improving speech recognition and understanding systems in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import struct\n",
    "import math\n",
    "from scipy.signal import butter, lfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxilliary functions for feature computation\n",
    "def spectral_selection(x, n):                              #out of the 19 subband energies computed, this function selects the n energies with the highest values\n",
    "    y = x.shape\n",
    "    row = y[0]\n",
    "    col = y[1]\n",
    "    xx = []\n",
    "    for i in range(0,col,1):\n",
    "        v = x[:,i]                                           # his line selects the i-th column of x and assigns it to the variable v\n",
    "        v = np.array([v])\n",
    "        v = v.T                 \n",
    "        t = np.array(np.arange(1,row+1)).reshape(-1,1)  #This line generates a column vector t containing values from 1 to row, representing the row numbers.\n",
    "\n",
    "        v = np.hstack((v, t))                      #This line horizontally stacks v and t, resulting in a matrix where the first column contains the values of v and the second column contains the row numbers.\n",
    "        v_sort = v[v[:,0].argsort(),]                 #This line sorts v based on the values in the first column, resulting in v_sort.\n",
    "        v_sort_sel = v_sort[row-n:row, :]             #This line selects the last n rows from v_sort and assigns the result to v_sort_sel.\n",
    "        vv = v_sort_sel[v_sort_sel[:,1].argsort(),]   #This line sorts v_sort_sel based on the values in the second column, resulting in vv\n",
    "        #tt = numpy.array([vv[:,0]])                  #The subsequent code block handles the concatenation of vv[:, 0] (the first column of vv) with the previous iterations' results stored in xx\n",
    "        if i!=0:\n",
    "            if i==1:\n",
    "                pp = np.array([xx])\n",
    "                pp = pp.T\n",
    "            else:\n",
    "                pp = xx\n",
    "            pp2 = np.array([vv[:,0]])\n",
    "            pp2 = pp2.T\n",
    "            xx = np.hstack((pp, pp2))\n",
    "        else:\n",
    "            xx = np.concatenate((xx, vv[:,0]))\n",
    "    return xx                                         # xx is a matrix containing the selected energies for all the frames with size n x col\n",
    "\n",
    "\n",
    "def temp_vec_corr(x2, t_sigma):\n",
    "    from scipy.stats import norm\n",
    "    y = x2.shape\n",
    "    row = y[0]\n",
    "    col = y[1]\n",
    "    wn = norm.pdf(np.arange(1,col+1,1), (col+1)/2, t_sigma)              # pdf function with mean = (col+1)/2 and std = t_sigma as window\n",
    "    # NOTE: if we use continue to manipulate the variable x2, (the function argument), then it gets reflected back in\n",
    "    # in the parent function. (No idea why). So create a copy of x2 and work with that.\n",
    "    x3 = np.zeros((row,col))\n",
    "    for i in range(0,row,1):\n",
    "        x3[i,:] = np.multiply(x2[i,:],wn)                                # windowing the frame energies\n",
    "    s=0\n",
    "    for i in range(0,col-1,1):\n",
    "        for j in range(i+1,col,1):\n",
    "            s+= np.multiply(x3[:,i], x3[:,j])                             # computing the correlation between the consecutive frames \n",
    "    if col!=1:\n",
    "        s = np.sqrt(np.divide(s, (col-1)*col/2))\n",
    "    else:\n",
    "        s = x3                                                             \n",
    "    return s\n",
    "\n",
    "def temporal_corr(x, win, t_sigma):\n",
    "    hwin = (win-1)/2           # hwin is the half window size\n",
    "    yy = x.shape\n",
    "    row = yy[0]\n",
    "    col = yy[1]\n",
    "\n",
    "    row = int(row)\n",
    "    hwin = int(hwin)\n",
    "\n",
    "    x = np.array([np.concatenate((np.zeros((row,hwin)), x, np.zeros((row, hwin))), axis = 1)])              # zero padding the input matrix, hwin zeros on each sides of the columns\n",
    "    y = []\n",
    "    for i in range(hwin,col+hwin,1):\n",
    "        temp2 = x[0,:,i-hwin:i+hwin+1]\n",
    "        z = temp_vec_corr(temp2, t_sigma)\n",
    "        z = np.array([z]).T\n",
    "        if i==hwin:\n",
    "            y = np.concatenate((y, z[:,0]))\n",
    "        else:\n",
    "            if i==hwin+1:\n",
    "                y = np.array([y]).T\n",
    "            y = np.hstack((y, z))\n",
    "    return y\n",
    "\n",
    "def spectral_corr(x):\n",
    "    yy = x.shape\n",
    "    row = yy[0]\n",
    "    col = yy[1]\n",
    "\n",
    "    s = np.zeros((1, col))\n",
    "    for i in range(0, row-1, 1):\n",
    "        for j in range(i+1, row, 1):\n",
    "            s = s+np.multiply(x[i,:], x[j,:])\n",
    "\n",
    "    if row!=1:\n",
    "        s = np.sqrt(np.divide(s, (row*(row-1)/2)))\n",
    "    else:\n",
    "        s = x\n",
    "    return s\n",
    "\n",
    "def statFunctions_Syl(t):\n",
    "    from scipy.stats.mstats import gmean\n",
    "    if np.min(t)<0:\n",
    "        t = np.subtract(t,min(t[0]))\n",
    "        #out = []\n",
    "        #return out\n",
    "    out = np.array([np.median(t[0]), np.mean(t[0]), gmean(np.absolute(t[0])), np.max(t[0])-np.min(t[0]), np.std(t[0])])\n",
    "    out = np.array([out]).T\n",
    "    t = np.subtract(t,np.min(t[0]))\n",
    "    t = np.divide(t, np.sum(t[0]))\n",
    "    tempArr = np.array([np.arange(1,len(t[0])+1)])\n",
    "    temporalMean = np.sum(np.multiply(tempArr,t)[0])\n",
    "    temporalStd = np.sqrt(np.sum(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),2),t[0])))\n",
    "    temporalSkewness = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),3),t[0]),np.power(temporalStd,3)))\n",
    "    temporalKurthosis = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),4),t[0]),np.power(temporalStd,4)))\n",
    "    arr1 = np.array([np.array([temporalStd, temporalSkewness, temporalKurthosis])]).T\n",
    "    out = np.vstack((out,arr1))\n",
    "    return out\n",
    "\n",
    "def statFunctions_Vwl(t):\n",
    "    if np.min(t)<0:\n",
    "        t = np.subtract(t,min(t[0]))\n",
    "        #out = []\n",
    "        #eturn out\n",
    "    out = np.array([np.median(t[0]), np.mean(t[0]), np.max(t[0])-np.min(t[0]), np.std(t[0])])\n",
    "    out = np.array([out]).T\n",
    "    t = np.subtract(t,np.min(t[0]))\n",
    "    t = np.divide(t, np.sum(t[0]))\n",
    "    tempArr = np.array([np.arange(1,len(t[0])+1)])\n",
    "    temporalMean = np.sum(np.multiply(tempArr,t)[0])\n",
    "    temporalStd = np.sqrt(np.sum(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),2),t[0])))\n",
    "    temporalSkewness = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),3),t[0]),np.power(temporalStd,3)))\n",
    "    temporalKurthosis = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),4),t[0]),np.power(temporalStd,4)))\n",
    "    arr1 = np.array([np.array([temporalStd, temporalSkewness, temporalKurthosis])]).T\n",
    "    out = np.vstack((out,arr1))\n",
    "    return out\n",
    "\n",
    "def smooth(t_cor, swin, sigma):\n",
    "    from scipy.stats import norm\n",
    "    ft = norm.pdf(np.arange(1,swin+1), (swin+1)/2, sigma)\n",
    "    ft = np.array([ft])\n",
    "    t_cor = np.array([t_cor])\n",
    "    convRes = np.zeros((1, t_cor.shape[2]+ft.shape[1]-1))\n",
    "    convRes = np.convolve(t_cor[0,0,:], ft[0,:])\n",
    "    y = convRes[np.arange((swin+1)//2-1, len(convRes)-(swin-1)//2, 1)]\n",
    "    return y\n",
    "\n",
    "def get_labels(lab_list,fa,fileName):\n",
    "        L=[]; fb=fa; filenm=[];\n",
    "        \n",
    "        for num in range(0,len(lab_list)):\n",
    "            if str((lab_list[num][0].tolist())[0]) == str('P'):\n",
    "                L.append(1)\n",
    "                filenm.append(fileName)           \n",
    "            else:\n",
    "                L.append(0)\n",
    "                filenm.append(fileName)\n",
    "        fb = np.vstack((fa,L))\n",
    "#        fb = np.vstack((fb,np.asarray(filenm,object)))\n",
    "        return fb,filenm\n",
    "\n",
    "def get_labels_seq2seq(lab_list):\n",
    "        L=[];# filenm=[];\n",
    "        \n",
    "        for num in range(0,len(lab_list)):\n",
    "            if str((lab_list[num][0].tolist())[0]) == str('P'):\n",
    "                L.append(1)\n",
    "#                filenm.append(fileName)           \n",
    "            else:\n",
    "                L.append(0)\n",
    "                #filenm.append(fileName)\n",
    "        #fb = np.vstack((fa,L))\n",
    "#        fb = np.vstack((fb,np.asarray(filenm,object)))\n",
    "        return L\n",
    "    \n",
    "def vocoder_func(wavPath):\n",
    "\n",
    "    # FILTER DEFINITIONS\n",
    "\n",
    "    def butter_bandpass(lowcut, highcut, fs, order):\n",
    "        nyq = 0.5*fs\n",
    "        low = float(lowcut) / nyq\n",
    "        high = float(highcut) / nyq\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        return b, a\n",
    "\n",
    "    def butter_lowpass(lowcut, fs, order):\n",
    "        nyq = 0.5*fs\n",
    "        low = float(lowcut) / nyq\n",
    "        b ,a = butter(order, low, btype='lowpass')\n",
    "        return b, a\n",
    "\n",
    "    def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        y = lfilter(b, a, data)\n",
    "        return y\n",
    "\n",
    "    def butter_lowpass_filter(data, lowcut, fs, order):\n",
    "        b, a = butter_lowpass(lowcut, fs, order=order)\n",
    "        y = lfilter(b, a, data)\n",
    "        return y\n",
    "\n",
    "    # FUNCTION TO READ A .wav FILE MATLAB STYLE\n",
    "\n",
    "    def readWav(wavPath):\n",
    "        waveFile = wave.open(wavPath)\n",
    "        fs = waveFile.getframerate()\n",
    "        length = waveFile.getnframes()\n",
    "        data = []\n",
    "        for i in range(0, length):\n",
    "            waveData = waveFile.readframes(1)\n",
    "            data.append(struct.unpack(\"<h\", waveData))\n",
    "        waveFile.close()\n",
    "        data = np.array([data])\n",
    "        data = data.astype(float)/np.max(np.abs(data))\n",
    "        data = data[0]\n",
    "        return data, fs, length\n",
    "\n",
    "    # BUFFER FUNCTION AS DEFINED IN MATLAB\n",
    "\n",
    "    def buffer(x, n, p=0, opt=None):\n",
    "        import numpy\n",
    "        if p >= n:\n",
    "            raise ValueError('p ({}) must be less than n ({}).'.format(p,n))\n",
    "        cols = int(numpy.ceil(len(x)/float(n-p)))+1\n",
    "        if opt == 'nodelay':\n",
    "            cols += 1\n",
    "        elif opt != None:\n",
    "            raise SystemError('Only `None` (default initial condition) and '\n",
    "                              '`nodelay` (skip initial condition) have been '\n",
    "                              'implemented')\n",
    "        b = numpy.zeros((n, cols))\n",
    "        j = 0\n",
    "        for i in range(cols):\n",
    "            if i == 0 and opt == 'nodelay':\n",
    "                b[0:n,i] = x[0:n]\n",
    "                continue\n",
    "            elif i != 0 and p != 0:\n",
    "                b[:p, i] = b[-p:, i-1]\n",
    "            else:\n",
    "                b[:p, i] = 0\n",
    "            k = j + n - p\n",
    "            n_end = p+len(x[j:k])\n",
    "            b[p:n_end,i] = x[j:k,0]\n",
    "            j = k\n",
    "        return b\n",
    "\n",
    "    fltcF= np.array([240,360,480,600,720,840,1000,1150,1300,1450,1600,1800,2000,2200,2400,2700,3000,3300,3750])\n",
    "    fltBW= np.array([120,120,120,120,120,120,150,150,150,150,150,200,200,200,200,300,300,300,500])\n",
    "\n",
    "    fltFc= np.array([np.subtract(fltcF,np.divide(fltBW,2)),np.add(fltcF,np.divide(fltBW,2))])\n",
    "    fltLpFc= 50\n",
    "\n",
    "    sig, Fs, length = readWav(wavPath)\n",
    "    # print(\"sig.shape: \", sig.shape)\n",
    "    # print(\"Fs: \", Fs)\n",
    "\n",
    "    # Saving the audio in a txt file\n",
    "    xx = np.append(Fs,sig)                       # sig is the amplitude of the audio signal\n",
    "\n",
    "    nWndw = int(round(Fs*0.02))\n",
    "    # print(\"nWndw is: \", nWndw)\n",
    "\n",
    "    nOverlap = int(round(Fs*0.01))\n",
    "    # print(\"nOverlap is: \", nOverlap)\n",
    "\n",
    "    sig = 0.99*sig/max(abs(sig))                 # Normalizing the signal\n",
    "    \n",
    "    # Windowing first and filtering next\n",
    "    sigFrames= buffer(sig*32768,nWndw,nOverlap)          # sig Frames is a 2D array where each column represents an analysis frame \n",
    "    subBandEnergies= np.zeros([19,sigFrames.shape[1]])   # 2D array with 19 rows and number of columns equal to number of frames\n",
    "\n",
    "    for j in range(0,sigFrames.shape[1]): \n",
    "        currFrame = np.array([sigFrames[:,j]])                  # 1D array with the selected frame\n",
    "        for i in range(0,fltFc.shape[1]):\n",
    "            fltFrame = butter_bandpass_filter(currFrame[0], fltFc[0][i], fltFc[1][i], Fs, 2); fltFrame = fltFrame.T  # this line applies the bandpass filter to the current frame with fltFc[0][i] as lowcut and fltFc[1][i] as highcut\n",
    "            rectFrame = np.abs(fltFrame[0:nWndw])\n",
    "            lpFltFrame = butter_lowpass_filter(rectFrame, float(fltLpFc), Fs, 2)\n",
    "            currEnergy = lpFltFrame[nWndw-1]\n",
    "            if currEnergy < 1:\n",
    "                currEnergy = 0.5\n",
    "            subBandEnergies[i,j] = math.exp(2*math.log(currEnergy)/math.log(10))\n",
    "    subBandEnergies = np.concatenate((np.exp(0.5*np.ones((19,1))),subBandEnergies[:,0:-2]),axis=1).T\n",
    "\n",
    "    return Fs, subBandEnergies, xx\n",
    "\n",
    "# so you have the signal in frames. Each column is a frame and the frames and are obtained by windowing the original signal and the frames have a 50% overlap.\n",
    "# you calculate the subbandenergies frame by frame \n",
    "# for each frame you bandpass filter it through the 19 subbandenergies and take log of the last value of the filtered signal as the sub-band energy value.\n",
    "# you concatenate a 0.5 to the first column of the subbandenergies matrix and remove the last two columns of the subbandenergies matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 180.  300.  420.  540.  660.  780.  925. 1075. 1225. 1375. 1525. 1700.\n",
      "  1900. 2100. 2300. 2550. 2850. 3150. 3500.]\n",
      " [ 300.  420.  540.  660.  780.  900. 1075. 1225. 1375. 1525. 1675. 1900.\n",
      "  2100. 2300. 2500. 2850. 3150. 3450. 4000.]]\n"
     ]
    }
   ],
   "source": [
    "fltcF= np.array([240,360,480,600,720,840,1000,1150,1300,1450,1600,1800,2000,2200,2400,2700,3000,3300,3750])\n",
    "fltBW= np.array([120,120,120,120,120,120,150,150,150,150,150,200,200,200,200,300,300,300,500])\n",
    "\n",
    "fltFc= np.array([np.subtract(fltcF,np.divide(fltBW,2)),np.add(fltcF,np.divide(fltBW,2))])\n",
    "fltLpFc= 50\n",
    "\n",
    "print(fltFc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import scipy\n",
    "from scipy.signal import medfilt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "ger_test_dir = os.path.join(data_dir, \"GER/test/\")\n",
    "ger_train_dir = os.path.join(data_dir, \"GER/train/\")\n",
    "# ita_test_dir = os.path.join(data_dir, \"ITA/test/\")\n",
    "# ita_train_dir = os.path.join(data_dir, \"ITA/train/\")\n",
    "\n",
    "phn_dir = os.path.join(data_dir, \"fisher-2000_FA_GT_ESTphnTrans_estStress/lab/txt/phn/\")\n",
    "dict_name = \"nativeEnglishDict_gt100_manoj.syl\"\n",
    "stressLabelspath = data_dir + \"FA_htkCorrectedLabWithFullAudio\" + \"/lab/mat/sylStress/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the directories exist\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Data directory does not exist\")\n",
    "if not os.path.exists(ger_train_dir):\n",
    "    print(\"German Train directory does not exist\")\n",
    "# if not os.path.exists(ita_train_dir):\n",
    "#     print(\"Italian Train directory does not exist\")\n",
    "if not os.path.exists(phn_dir):\n",
    "    print(\"Phoneme directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_test_files = os.listdir(ger_test_dir)\n",
    "ger_train_files = os.listdir(ger_train_dir)\n",
    "# ita_test_files = os.listdir(ita_test_dir)\n",
    "# ita_train_files = os.listdir(ita_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features\n",
    "twin = 5\n",
    "t_sigma = 1.4\n",
    "swin = 7\n",
    "s_sigma = 1.5\n",
    "mwin = 13\n",
    "max_threshold = 25\n",
    "\n",
    "vwlSB_num = 4\n",
    "vowelSB = [1, 2, 4, 5, 6, 7, 8, 13, 14, 15, 16, 17]\n",
    "sylSB_num = 5\n",
    "sylSB = [1, 2, 3, 4, 5, 6, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "startWordFrame_all = []\n",
    "spurtStartFrame_all = []\n",
    "spurtEndFrame_all = []\n",
    "vowelStartFrame_all = []\n",
    "vowelEndFrame_all = []\n",
    "eng_full_all = []\n",
    "spurtStress_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_array(phn_file):\n",
    "    data_array = []\n",
    "    try:\n",
    "        fid = open(phn_file, 'r')\n",
    "        data_array = np.loadtxt(fid, dtype={'names': ('a', 'b', 'c'), 'formats': ('f4', 'f4', 'S16')})\n",
    "        fid.close\n",
    "    except:\n",
    "        print('File does not exist')\n",
    "        return\n",
    "\n",
    "    ghastly = []\n",
    "    for i in range(len(data_array)):\n",
    "        tuple_list = list(data_array[i])\n",
    "        tuple_list[2] = tuple_list[2].decode()\n",
    "        ghastly.append((tuple_list[0], tuple_list[1], tuple_list[2]))\n",
    "    return np.array(ghastly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phone_data(data_array):\n",
    "    phnTimes1 = [row[0] for row in data_array]\n",
    "    phnTimes1 = np.array([phnTimes1]).T\n",
    "\n",
    "    phnTimes2 = [row[1] for row in data_array]\n",
    "    phnTimes2 = np.array([phnTimes2]).T\n",
    "\n",
    "    phnTimes = np.hstack((phnTimes1, phnTimes2))\n",
    "    phones = [row[2] for row in data_array]\n",
    "    phones = np.array([phones])\n",
    "\n",
    "    # Made them lowercase since the syl dictionary is in lowercase\n",
    "    for i in range(0, len(phones[0])):\n",
    "        phones[0][i] = phones[0][i].lower()\n",
    "        \n",
    "    origPhones = phones\n",
    "    index = np.argwhere(origPhones[0] == 'sil')\n",
    "    phones = phones[phones != 'sil']\n",
    "    phones = np.array([phones])\n",
    "    phones = phones.reshape(1, -1)\n",
    "\n",
    "    phnTimes2 = np.delete(phnTimes2, index, axis=0)\n",
    "    phnTimes = np.delete(phnTimes, index, axis=0)\n",
    "    \n",
    "    return phones, phnTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vowel data\n",
    "def get_vowel_data(data_array):\n",
    "    # VOWEL LIST\n",
    "    vowelList = ['aa', 'ae', 'ah', 'ao', 'aw', 'ay', 'eh', 'er', 'ey', 'ih', 'iy', 'ow', 'oy', 'uh', 'uw']\n",
    "    vowel_start_time = []\n",
    "    vowel_end_time = []\n",
    "    vowel = []\n",
    "\n",
    "    for i in range(0, len(data_array)):\n",
    "        if data_array[i][2].lower() in vowelList:\n",
    "            vowel_start_time.append(data_array[i][0])\n",
    "            vowel_end_time.append(data_array[i][1])\n",
    "            vowel.append(data_array[i][2].lower())\n",
    "            \n",
    "    vowel_start_time = np.array([vowel_start_time])\n",
    "    vowel_end_time = np.array([vowel_end_time])\n",
    "    vowel = np.array([vowel])\n",
    "    return vowel, vowel_start_time, vowel_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_name):\n",
    "    # Define the path to the transcript file\n",
    "    trans_path = data_dir + \"ISLEtrans.txt\"\n",
    "\n",
    "    # Read the contents of the transcript file\n",
    "    with open(trans_path, 'r') as trans_file:\n",
    "        trans_contents = trans_file.read()\n",
    "\n",
    "    # Extract the lines containing the specified filename from the transcript\n",
    "    lines = [line for line in trans_contents.split('\\n') if file_name in line]\n",
    "\n",
    "    # Extract the words from the lines and clean them up\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        _, word_list = line.split(' ', 1)\n",
    "        words.extend(re.findall(r'\\b\\w+\\b', word_list))\n",
    "    words = [word.lower() for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_syls(words):\n",
    "    d = collections.defaultdict(list)\n",
    "    with open(data_dir + dict_name, 'r') as f:\n",
    "        for line in f:\n",
    "            key = line.split()[0]\n",
    "            val = line.split('=')[1].strip()\n",
    "            d[key].append(val)\n",
    "\n",
    "    word_syls = []\n",
    "    for i in range(len(words)):\n",
    "        curr_word_syls = []\n",
    "        if words[i] in d:\n",
    "            curr_word_syls = d[words[i]]\n",
    "        word_syls.append(curr_word_syls)\n",
    "    return word_syls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_indices(words, word_syls, phones):\n",
    "    newSuccessInds_all = []\n",
    "    newSuccessInds_all2 = []\n",
    "\n",
    "    prevSuccessInds_all = []\n",
    "    prevSuccessInds_all.append(0)\n",
    "\n",
    "    # I said white not bait\n",
    "    for iterWord in range(0, len(words)):\n",
    "        currWordSyls = word_syls[iterWord]   #\n",
    "        countSuccess = 1\n",
    "\n",
    "        for iterPrev in range(0, len(prevSuccessInds_all)):\n",
    "            prevWordSyls = \"\"\n",
    "            if prevSuccessInds_all[iterPrev] == 0:\n",
    "                currPrevSylInds = []\n",
    "            else:\n",
    "                currPrevSylInds = prevSuccessInds_all[iterPrev]\n",
    "                for iterPrevSyls in range(0, len(currPrevSylInds)):\n",
    "                    temp = word_syls[iterPrevSyls]\n",
    "                    prevWordSyls = prevWordSyls + \\\n",
    "                        temp[currPrevSylInds[iterPrevSyls]]+\" \"\n",
    "\n",
    "            # iterating through the syllables of the current word\n",
    "            for iterCurr in range(0, len(currWordSyls)):\n",
    "                currTestWordSyls = prevWordSyls + currWordSyls[iterCurr]\n",
    "                temp2 = currTestWordSyls.replace(' . ', ' ')\n",
    "                \n",
    "                \n",
    "                inds = [m.start() for m in re.finditer(' ', temp2)]\n",
    "                if len(inds) == 0:\n",
    "                    inds = [len(temp2)]\n",
    "\n",
    "                count = 1\n",
    "                temp = []\n",
    "\n",
    "                for iterTemp in range(len(inds)):\n",
    "                    if iterTemp == 0:\n",
    "                        temp1 = temp2[0:inds[iterTemp]]\n",
    "                        # print(temp2 + \"\\t\\t| \" + temp1)\n",
    "                    else:\n",
    "                        temp1 = temp2[inds[iterTemp-1]+1:inds[iterTemp]]\n",
    "                    if not ((np.unique(temp1) == ' ').any() or (len(temp1) == 0)):\n",
    "                        temp.append(temp1)\n",
    "                        count += 1\n",
    "                        \n",
    "                if iterTemp == len(inds) - 1 and len(inds) < len(currTestWordSyls):\n",
    "                    temp1 = temp2[inds[iterTemp]+1:len(temp2)]\n",
    "                    if not ((len(temp1) == 0) or (np.unique(temp1) == ' ').any()):\n",
    "                        temp.append(temp1)\n",
    "                        count = count+1\n",
    "\n",
    "                if iterWord + 1 == len(words):\n",
    "                    currPhones = phones[0, 0:len(phones[0])]\n",
    "                else:\n",
    "                    currPhones = phones[0][0:len(temp)]\n",
    "\n",
    "                    \n",
    "                flag = 1\n",
    "                for iterFlag in range(0, len(currPhones), 1):\n",
    "                    if len(currPhones) != len(temp):\n",
    "                        flag = 0\n",
    "                    else:\n",
    "                        if currPhones[iterFlag] != temp[iterFlag]:\n",
    "                            flag = 0\n",
    "                if flag == 1:\n",
    "                    if not currPrevSylInds == []:\n",
    "                        for i in range(0, len(currPrevSylInds)):\n",
    "                            #                            print('line 122::::::yes')\n",
    "                            newSuccessInds_all.append(currPrevSylInds[i])\n",
    "                    newSuccessInds_all.append(iterCurr)\n",
    "                    newSuccessInds_all2.append(newSuccessInds_all)\n",
    "                    newSuccessInds_all = []\n",
    "                    countSuccess = countSuccess+1\n",
    "                    \n",
    "        prevSuccessInds_all = newSuccessInds_all2\n",
    "        newSuccessInds_all2 = []\n",
    "    if len(prevSuccessInds_all) == 0:\n",
    "        return None, None\n",
    "    return prevSuccessInds_all[0], currTestWordSyls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syls_count(path_indices, currTestWordSyls, words, word_syls):\n",
    "    sylCount = 1\n",
    "    phnCount = 1\n",
    "    spurtSyl = []  # spurtSylTimes= np.zeros((len(phnTimes),2))\n",
    "\n",
    "    syls_word = np.zeros((1, len(path_indices)))\n",
    "    spurtWordTimes = np.zeros((len(path_indices), 2))                 # dont need this in this function \n",
    "\n",
    "    for iterPath in range(0, len(path_indices)):\n",
    "        # current word and syllables\n",
    "        currWord = words[iterPath]                                    # current word   \n",
    "        currWordSyls = word_syls[iterPath]                            # current word syllables\n",
    "\n",
    "        currSyl = currWordSyls[path_indices[iterPath]]                # current syllable\n",
    "        currSyl = currSyl.replace(' . ', '.')                         # replace ' . ' with '.'\n",
    "        # print(currSyl)\n",
    "        inds = [m.start() for m in re.finditer('\\.', currSyl)]        # indices of periods in the syllable \n",
    "\n",
    "        if len(inds) == 0:                                            # if there are no periods in the syllable\n",
    "            inds = [len(currSyl)]                                     # then the last index is the length of the syllable\n",
    "\n",
    "        count = 0\n",
    "        for iterTemp in range(0, len(inds)):                          # iterating through the indices of the periods in the syllable\n",
    "            if iterTemp == 0:\n",
    "                temp1 = currSyl[0:inds[iterTemp]]                     # temp1 is the substring from the beginning of the syllable to the first period\n",
    "            else:\n",
    "                temp1 = currSyl[inds[iterTemp-1]+1:inds[iterTemp]]    # temp1 is the substring from the previous period to the current period\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):                 # if temp1 is not a space or empty\n",
    "                spurtSyl.append(temp1)                                # append temp1 to spurtSyl\n",
    "                sylCount = sylCount + 1                               # increment sylCount\n",
    "                count = count + 1                                     \n",
    "                \n",
    "        if iterTemp is len(inds)-1 and len(inds) < len(currTestWordSyls):   \n",
    "            temp1 = currSyl[inds[iterTemp]+1:len(currSyl)]            # temp1 is the substring from the last period to the end of the syllable\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):                 # if temp1 is not a space or empty\n",
    "                spurtSyl.append(temp1)                                # append temp1 to spurtSyl\n",
    "                sylCount = sylCount + 1                               # increment sylCount\n",
    "                count = count + 1                                     # increment count\n",
    "        syls_word[0][iterPath] = count                                # syls_word is a 1D array with the number of syllables in each word\n",
    "\n",
    "    return syls_word, spurtSyl, spurtWordTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spurts(spurtSyl, currTestWordSyls, phnTimes):\n",
    "    phnCount = 1\n",
    "    spurtSylTimes = np.zeros((len(spurtSyl), 2))\n",
    "\n",
    "    for iterSyl in range(0, len(spurtSyl)):                               # iterating through the syllables in the spurt\n",
    "        temp2 = spurtSyl[iterSyl]                                         # temp2 is the current syllable\n",
    "        inds = [m.start() for m in re.finditer(' ', temp2)]               # indices of spaces in the syllable\n",
    "        if len(inds) == 0:                                                # if there are no spaces in the syllable\n",
    "            inds = [len(temp2)]                                           # then the last index is the length of the syllable\n",
    "        count = 1                                                         # count is the number of phonemes in the syllable\n",
    "        temp = []                                                         # temp is a list of phonemes in the syllable\n",
    "        for iterTemp in range(0, len(inds)):                              # iterating through the indices of the spaces in the syllable\n",
    "            if iterTemp == 0:                                             # if it is the first index\n",
    "                temp1 = temp2[0:inds[iterTemp]]                           # temp1 is the substring from the beginning of the syllable to the first space\n",
    "            else:                                                         # if it is not the first index\n",
    "                temp1 = temp2[inds[iterTemp-1]+1:inds[iterTemp]]          # temp1 is the substring from the previous space to the current space\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):                     # if temp1 is not a space or empty\n",
    "                temp.append(temp1)                                        # append temp1 to temp\n",
    "                count = count+1                                           # increment count\n",
    "        if iterTemp == len(inds)-1 and len(inds) < len(currTestWordSyls): # if it is the last index and there are less spaces than syllables\n",
    "            temp1 = temp2[inds[iterTemp]+1:len(temp2)]                    # temp1 is the substring from the last space to the end of the syllable\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):                     # if temp1 is not a space or empty\n",
    "                temp.append(temp1)                                        # append temp1 to temp\n",
    "                count = count+1                                           # increment count\n",
    "\n",
    "        nPhns_syl = len(temp)                                             # nPhns_syl is the number of phonemes in the syllable\n",
    "        spurtSylTimes[iterSyl, 0] = phnTimes[phnCount-1, 0]               # the start time of the syllable is the start time of the first phoneme in the syllable\n",
    "        phnCount = phnCount + nPhns_syl                                   # increment phnCount by the number of phonemes in the syllable\n",
    "        spurtSylTimes[iterSyl, 1] = phnTimes[phnCount-1-1, 1]             # the end time of the syllable is the end time of the last phoneme in the syllable\n",
    "    return spurtSylTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spurt_word_times(path_indices, syls_word, spurtSylTimes):\n",
    "    spurtWordTimes = np.zeros((len(path_indices), 2))\n",
    "    sylIdx = 1\n",
    "\n",
    "    for iterWordTimes in range(0, len(syls_word[0])):                        # iterating through the number of syllables in each word\n",
    "        spurtWordTimes[iterWordTimes][0] = spurtSylTimes[sylIdx-1][0]        # spurtWordTimes is a 2D array with the start and end times of each word \n",
    "        sylIdx = sylIdx + syls_word[0][iterWordTimes].astype(int)            # sylIdx is the index of the last syllable of the current word\n",
    "        spurtWordTimes[iterWordTimes][1] = spurtSylTimes[sylIdx-1-1][1]      # sylIdx-1-1 is the index of the last phoneme of the last syllable of the current word\n",
    "    length_spurtWordTimes = iterWordTimes + 1\n",
    "    return spurtWordTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_boundaries(spurtWordTimes, words, spurtSylTimes):\n",
    "    # Processing word boundary file\n",
    "    # FILE READ DELETED HERE\n",
    "    a = spurtWordTimes\n",
    "    b = words\n",
    "    if (len(a) is not len(b)):\n",
    "        print(\"error\")\n",
    "    wordData = np.hstack((a, np.array([b], dtype='S32').T))\n",
    "    # print(wordData)\n",
    "\n",
    "    # Extract first coloumn of wordData\n",
    "    startWordTime = [row[0] for row in wordData]\n",
    "    endWordTime = [row[1] for row in wordData]\n",
    "\n",
    "    startWordFrame = np.round((np.subtract(np.array(startWordTime, dtype='float'), spurtSylTimes[0][0].astype(float))*100))\n",
    "    endWordFrame = np.round((np.subtract(np.array(endWordTime, dtype='float'), spurtSylTimes[0][0].astype(float))*100) + 1)\n",
    "    startWordFrame = np.append(startWordFrame, endWordFrame[-1])\n",
    "\n",
    "    return startWordFrame, endWordFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowelStartTime, startWordFrame):\n",
    "    # TCSSBC computation\n",
    "    # vowelStartTime = np.squeeze(vowelStartTime)        #NOTE you changed this, not in original code \n",
    "    startWordFrame = np.squeeze(startWordFrame)        #NOTE you changed this, not in original code \n",
    "\n",
    "    if len(sylSB) > sylSB_num:\n",
    "        eng = spectral_selection(\n",
    "            eng_full[np.subtract(sylSB, 1), :], sylSB_num)\n",
    "    else:\n",
    "        eng = eng_full[sylSB, :]               # extract only the sub-band energies that are in the sylSB list\n",
    "\n",
    "    # print(\"eng.shape: \", eng.shape)\n",
    "\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)             # calculate correlation spectrally and temporally \n",
    "    # print(\"t_cor.shape: \", t_cor.shape)\n",
    "    # print(\"t_cor\")\n",
    "    # print(t_cor)\n",
    "\n",
    "    s_cor = spectral_corr(t_cor)\n",
    "    # print(\"s_cor.shape: \", s_cor.shape)\n",
    "    # print(\"s_cor\")\n",
    "    # print(s_cor)\n",
    "\n",
    "    sylTCSSBC = smooth(s_cor, swin, s_sigma)                # smooth the correlation\n",
    "    # print(sylTCSSBC)\n",
    "    sylTCSSBC = np.array([sylTCSSBC])                       \n",
    "    # print(\"sylTCSSBC.shape: \", sylTCSSBC.shape)\n",
    "    # print(\"sylTCSSBC\")\n",
    "    # print(sylTCSSBC)\n",
    "\n",
    "    start_idx = np.round(spurtStartTime[0]*100).astype(int)             # get the start index of the spurt\n",
    "    # print(\"start_idx: \", start_idx)\n",
    "\n",
    "    sylTCSSBC = np.array([sylTCSSBC[0][start_idx:-1]])                  # clip the TCSSBC contour from the spurt start\n",
    "    # print(\"sylTCSSBC.shape: \", sylTCSSBC.shape)\n",
    "\n",
    "    sylTCSSBC = np.divide(sylTCSSBC, max(sylTCSSBC[0]))                 # normalize the TCSSBC contour\n",
    "\n",
    "    if len(vowelSB) > vwlSB_num:                                       \n",
    "        eng = spectral_selection(eng_full[np.subtract(vowelSB, 1), :], vwlSB_num)\n",
    "    else:\n",
    "        eng = eng_full[vowelSB, :]                                       # extract only the sub-band energies that are in the vowelSB list\n",
    "\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)\n",
    "    s_cor = spectral_corr(t_cor)                     \n",
    "    vwlTCSSBC = smooth(s_cor, swin, s_sigma)      \n",
    "\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC])\n",
    "\n",
    "    # Modify TCSSBC contour by clipping from the vowel start\n",
    "    start_idx = np.round(vowelStartTime[0][0]*100).astype(int)         # get the start index of the vowel\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC[0][start_idx:-1]])                 # clip the TCSSBC contour from the vowel start\n",
    "\n",
    "    vwlTCSSBC = np.divide(vwlTCSSBC, max(vwlTCSSBC[0]))                 # normalize the TCSSBC contour\n",
    "    # print(\"vwlTCSSBC.shape: \", vwlTCSSBC.shape)\n",
    "\n",
    "    # Compute silence statistics\n",
    "    # Preprocessing of the data\n",
    "    # print(\"sylTCSSBC\", sylTCSSBC)\n",
    "    # print(\"vwlTCSSBC\", vwlTCSSBC)\n",
    "    # print(sylTCSSBC.shape)\n",
    "    # print(vwlTCSSBC.shape)\n",
    "\n",
    "     \n",
    "    word_duration = np.zeros((1, len(startWordFrame) - 1))\n",
    "    # print(startWordFrame.shape)\n",
    "    # print(startWordFrame)\n",
    "    # print(len(startWordFrame) - 1)\n",
    "    # print(\"word_duration.shape: \", word_duration.shape)\n",
    "    word_Sylsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "    word_Vwlsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "    \n",
    "    for j in range(0, len(startWordFrame) - 1):\n",
    "        temp_start = startWordFrame[j].astype(int)\n",
    "        temp_end = startWordFrame[j + 1].astype(int) - 1\n",
    "        # print(\"temp_start: \", temp_start)\n",
    "        # print(\"temp_end: \", temp_end)\n",
    "        # jhansi\n",
    "        if (temp_end >= sylTCSSBC.shape[1]):\n",
    "            temp_end1 = sylTCSSBC.shape[1]-1\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end1)] = medfilt(sylTCSSBC[0, np.arange(temp_start, temp_end1)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end1] = sylTCSSBC[0, temp_end1 - 1]                          # median filteringv the TCSSBC contour\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end1)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)                                        # calculate the sum of the TCSSBC contour for the word  # need word boundaries for this \n",
    "        else:\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "                sylTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end] = sylTCSSBC[0, temp_end - 1]\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)\n",
    "            \n",
    "        if (temp_end >= vwlTCSSBC.shape[1]):\n",
    "            temp_end = vwlTCSSBC.shape[1]-1\n",
    "\n",
    "        # temp_end = np.min([temp_end,len(vwlTCSSBC)])\n",
    "        vwlTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "            vwlTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "        vwlTCSSBC[0, temp_start] = vwlTCSSBC[0, temp_start+1]\n",
    "        vwlTCSSBC[0, temp_end] = vwlTCSSBC[0, temp_end - 1]\n",
    "\n",
    "        word_duration[0, j] = temp_end - temp_start + 1                      # calculate the duration of the word in frames ## need word boundaries for this\n",
    "\n",
    "        tempArr = vwlTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "        word_Vwlsum[0, j] = tempArr.sum(axis=0)                              # calculate the sum of the TCSSBC contour for the word  # need word boundaries for this\n",
    "\n",
    "    sylTCSSBC[np.isnan(sylTCSSBC)] = 0   # Feature vector 1\n",
    "    vwlTCSSBC[np.isnan(vwlTCSSBC)] = 0   # Feature vector 2\n",
    "    return sylTCSSBC, vwlTCSSBC, word_Sylsum, word_duration\n",
    "\n",
    "# you need word duration keep it okay bye "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_feature_contour(sylTCSSBC, spurtStartFrame, spurtEndFrame):\n",
    "#     # Chunking the feature contour\n",
    "#     sylTCSSBC_chunk = []\n",
    "#     for i in range(0, len(spurtStartFrame)):\n",
    "#         sylTCSSBC_chunk.append(sylTCSSBC[int(spurtStartFrame[i]):int(spurtEndFrame[i])])\n",
    "#     return np.array(sylTCSSBC_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_contour(i, wav_file, test_data):\n",
    "    file_name = wav_file[:-4]\n",
    "    # print(\"file_name\", file_name)\n",
    "    phn_file = phn_dir + file_name + \".txt\"\n",
    "    # print(\"phn_file\", phn_file)\n",
    "    mat_file = stressLabelspath + file_name + \".mat\"\n",
    "    # print(\"mat_file\", mat_file)\n",
    "\n",
    "    if not os.path.exists(phn_file):\n",
    "        print(\"phn file doesn't exist\")\n",
    "        return None, False\n",
    "    \n",
    "    if not os.path.exists(mat_file):\n",
    "        print(\"mat file doesn't exist\")\n",
    "        return None, False\n",
    "\n",
    "    data_array = get_data_array(phn_file)\n",
    "    print(\"data array\", data_array)\n",
    "    phones, phn_times = get_phone_data(data_array)\n",
    "    print(\"phones\", phones)\n",
    "    print(\"phn_times\", phn_times)\n",
    "    vowel, vowel_start_time, vowel_end_time = get_vowel_data(data_array)\n",
    "    print(\"vowel\", vowel)\n",
    "    print(\"vowel_start_time\", vowel_start_time)\n",
    "    print(\"vowel_end_time\", vowel_end_time)\n",
    "    words = get_words(file_name)\n",
    "    print(\"words\", words)\n",
    "    word_syls = get_word_syls(words)\n",
    "    print(\"word_syls\", word_syls)\n",
    "    path_indices, currTestWordSyls = get_path_indices(words, word_syls, phones)    \n",
    "    print(\"path_indices\", path_indices) \n",
    "    print(\"currTestWordSyls\", currTestWordSyls)\n",
    "\n",
    "    if path_indices == None:\n",
    "        return None, False\n",
    "\n",
    "    syls_word, spurtSyl, spurtWordTimes = get_syls_count(path_indices, currTestWordSyls, words, word_syls)\n",
    "    print(\"syls_word\", syls_word)\n",
    "    print(\"spurtSyl\", spurtSyl)\n",
    "    print(\"spurtWordTimes\", spurtWordTimes)\n",
    "\n",
    "    spurtSylTimes = get_spurts(spurtSyl, currTestWordSyls, phn_times)\n",
    "    print(\"spurtSylTimes\", spurtSylTimes)\n",
    "\n",
    "    syls_word = syls_word.astype('i')\n",
    "    print(\"syls_word\", syls_word)\n",
    "\n",
    "    spurtWordTimes = get_spurt_word_times(path_indices, syls_word, spurtSylTimes)\n",
    "    print(\"spurtWordTimes\", spurtWordTimes)\n",
    "\n",
    "    # Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\n",
    "    file_dir = ger_test_dir if test_data else ger_train_dir\n",
    "    Fs, eng_full, xx = vocoder_func(file_dir + wav_file)\n",
    "    # print(\"eng_full\", eng_full)     # eng_full contains the sub-band energies of the audio file\n",
    "    # print(\"xx\", xx)                 # xx contains the amplitude of the waveform of the audio file\n",
    "    eng_full = eng_full.conj().transpose()\n",
    "    # print(\"eng_full\", eng_full)\n",
    "    # print(eng_full.shape) \n",
    "\n",
    "    startWordFrame, endWordFrame = process_word_boundaries(spurtWordTimes, words, spurtSylTimes)\n",
    "    # print(\"startWordFrame\", startWordFrame)                         # startWordFrame contains the start frame of each word\n",
    "    # print(\"endWordFrame\", endWordFrame)                             # endWordFrame contains the end frame of each word\n",
    "    \n",
    "    # Processing of stress and syllable boundary file\n",
    "    spurtSylTime = spurtSylTimes\n",
    "    spurtStartTime = spurtSylTime[:, 0]\n",
    "    spurtEndTime = spurtSylTime[:, 1]\n",
    "    spurtStartFrame = np.round((spurtStartTime - spurtStartTime[0]) * 100)\n",
    "    # print(\"spurtStartFrame\", spurtStartFrame)\n",
    "    spurtEndFrame = np.round((spurtEndTime - spurtStartTime[0]) * 100)\n",
    "    # print(\"spurtEndFrame\", spurtEndFrame)\n",
    "\n",
    "    # Processing of Vowel boundary file\n",
    "    vowel_start_time = vowel_start_time.astype(float)\n",
    "    vowel_end_time = vowel_end_time.astype(float)\n",
    "\n",
    "    vowelStartFrame = np.round(vowel_start_time*100 - spurtStartTime[0]*100)   # vowelStartFrame contains the start frame of each vowel\n",
    "    vowelEndFrame = np.round(vowel_end_time*100 - spurtStartTime[0]*100)       # vowelEndFrame contains the end frame of each vowel\n",
    "    # print(\"vowelStartFrame\", vowelStartFrame)\n",
    "    # print(\"vowelEndFrame\", vowelEndFrame)\n",
    "\n",
    "    # print(\"words = \", words)\n",
    "    # print(\"spurt start time = \", spurtStartTime, len(spurtStartTime))\n",
    "    # print(\"vowel start time = \", vowel_start_time, len(vowel_start_time[0]))\n",
    "    # print(\"start word frame = \", startWordFrame, len(startWordFrame))\n",
    "    # print(\"spurt start frame = \", spurtStartFrame, len(spurtStartFrame))\n",
    "    # print(\"spurt end frame = \", spurtEndFrame, len(spurtEndFrame))\n",
    "\n",
    "    # polym = []\n",
    "    # poly = []\n",
    "    # for n in syls_word[0]:\n",
    "    #     if n == 1:\n",
    "    #         polym.append(False)\n",
    "    #         poly.append(False)\n",
    "    #     else:\n",
    "    #         polym.extend([True] * n)\n",
    "    #         poly.append(True)\n",
    "    # num_poly = np.sum(polym)\n",
    "    # print(\"num_poly\", num_poly)\n",
    "    # if num_poly == 0:\n",
    "    #     return None, False\n",
    "    # print()\n",
    "\n",
    "    # # eliminate the monosyllabic words using the poly list\n",
    "    # syls_word = syls_word[0][poly]\n",
    "    # print(\"syls_word\", syls_word)\n",
    "\n",
    "    # spurtStartTime = spurtStartTime[polym]\n",
    "    # print(\"spurtStartTime\", spurtStartTime)\n",
    "\n",
    "    # vowel_start_time = np.array([vowel_start_time[0][polym]])\n",
    "    # print(\"vowel_start_time\", vowel_start_time)\n",
    "\n",
    "    # startWordFrame = startWordFrame[poly.append(True)]\n",
    "    # print(\"startWordFrame\", startWordFrame)\n",
    "\n",
    "    # spurtStartFrame = spurtStartFrame[polym]\n",
    "    # print(\"spurtStartFrame\", spurtStartFrame)\n",
    "\n",
    "    # spurtEndFrame = spurtEndFrame[polym]\n",
    "    # print(\"spurtEndFrame\", spurtEndFrame)\n",
    "\n",
    "    # print()\n",
    "    # print(\"words = \", words)\n",
    "    # print(\"spurt start time = \", spurtStartTime, len(spurtStartTime))\n",
    "    # print(\"vowel start time = \", vowel_start_time, len(vowel_start_time))\n",
    "    # print(\"start word frame = \", startWordFrame, len(startWordFrame))\n",
    "    # print(\"spurt start frame = \", spurtStartFrame, len(spurtStartFrame))\n",
    "    # print(\"spurt end frame = \", spurtEndFrame, len(spurtEndFrame))\n",
    "\n",
    "    sylTCSSBC, vwlTCSSBC, word_duration, word_Sylsum = get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowel_start_time, startWordFrame)         # NOTE\n",
    "    \n",
    "    # print(\"sylTCSSBC\", sylTCSSBC)\n",
    "    # print(sylTCSSBC.shape)\n",
    "    # print(\"vwlTCSSBC\", vwlTCSSBC)\n",
    "    # print(vwlTCSSBC.shape)\n",
    "    # print(\"word_duration\", word_duration)\n",
    "    # print(\"word_Sylsum\", word_Sylsum)\n",
    "    \n",
    "    # sylTCSSBC_chunk = chunk_feature_contour(sylTCSSBC, spurtStartFrame, spurtEndFrame)\n",
    "  \n",
    "    tempOut = np.array([[]])\n",
    "            \n",
    "    wordIndication = []\n",
    "    peakVals = []\n",
    "    avgVals = []\n",
    "    \n",
    "    # Generating the features\n",
    "    for j in range(0, len(spurtSyl), 1):\n",
    "            inds = (startWordFrame <= spurtStartFrame[j]).nonzero()   # finds the word that the syllable belongs to\n",
    "            word_ind = inds[0][-1]                           # finds the index of the word that the syllable belongs to\n",
    "            wordIndication.append(word_ind)                  # stores the index of the word that the syllable belongs to\n",
    "    #       print([0, np.arange(spurtStartFrame[j], spurtEndFrame[j]-1, 1).astype(int)])\n",
    "            currFtr1SylSeg = sylTCSSBC[0, np.arange(spurtStartFrame[j], spurtEndFrame[j]-1, 1).astype(int)]  # extracts the syllable segment from the TCSSBC contour\n",
    "            currFtr1SylSeg = np.array([currFtr1SylSeg])\n",
    "            temp = np.multiply(currFtr1SylSeg, len(currFtr1SylSeg[0]) / word_duration[0, word_ind])  # normalizes the syllable segment by the duration of the syllable and the duration of the word            # need word duration for this \n",
    "            arrResampled = np.array([librosa.resample(temp[0], Fs, Fs*float(30) / len(temp[0]), 'sinc_best')])       ##change   # resamples the syllable segment to 30 frames\n",
    "            \n",
    "            F_new = Fs*float(30) / len(temp[0])      ##change   # resampling frequency\n",
    "                                       \n",
    "            #To be put in the output file\n",
    "            peakVals.append(np.amax(arrResampled))\n",
    "            avgVals.append(np.average(arrResampled))\n",
    "        \n",
    "            currSylFtrs = statFunctions_Syl(arrResampled)   # calculates the statistical features of the syllable segment\n",
    "            arr1 = np.array([np.array([np.sum(currFtr1SylSeg) / word_Sylsum[0, word_ind]])]).T     # calculates ratio of the area under the TCSSBC contour of the syllable segment and the area under the TCSSBC contour for the word\n",
    "            currSylFtrs = np.vstack((currSylFtrs, arr1))     # appends the sum of the TCSSBC contour for the word to the statistical features of the syllable segment\n",
    "            #########jhansi\n",
    "            if (j>= vowelEndFrame.shape[1]):\n",
    "                break\n",
    "            if (vowelEndFrame [0,j] >= vwlTCSSBC.shape[1]):\n",
    "                vowelEndFrame[0,j] = vwlTCSSBC.shape[1]-1\n",
    "        \n",
    "            currFtr1VowelSeg = vwlTCSSBC[0, np.arange(vowelStartFrame[0, j], vowelEndFrame[0, j]-1, 1).astype(int)]   # extracts the vowel segment from the TCSSBC contour\n",
    "            currFtr1VowelSeg = np.array([currFtr1VowelSeg])\n",
    "            temp = np.multiply(currFtr1VowelSeg, len(currFtr1VowelSeg[0]) / word_duration[0, word_ind])  # normalizes the vowel segment by the duration of the syllable\n",
    "            if (len(temp[0])==0):\n",
    "                break\n",
    "                \n",
    "            arrResampled = np.array([librosa.resample(temp[0], F_new, F_new*float(20) / len(temp[0]), 'sinc_best')])     ##change  # resamples the vowel segment to 20 frames\n",
    "            currVowelFtrs = statFunctions_Vwl(arrResampled)      # calculates the statistical features of the vowel segment\n",
    "            arr1 = np.array([np.array([np.sum(currFtr1VowelSeg) / word_Sylsum[0, word_ind]])]).T        # calculates ratio of the area under the TCSSBC contour of the vowel segment and the area under the TCSSBC contour for the word\n",
    "            currVowelFtrs = np.vstack((currVowelFtrs, arr1))\n",
    "            if j == 0:\n",
    "                tempOut = np.vstack((currSylFtrs, currVowelFtrs, len(currFtr1VowelSeg[0]), len(currFtr1SylSeg[0])))    \n",
    "            else:\n",
    "                tempOut = np.hstack((tempOut, np.vstack((currSylFtrs, currVowelFtrs,len(currFtr1VowelSeg[0]), len(currFtr1SylSeg[0])))))                # tempOut columns contain the statistical features of the syllable segment and the vowel segment, the duration of the vowel segment and the duration of the syllable segment\n",
    "\n",
    "    if (len(temp[0])==0):\n",
    "            return None, False   ###\n",
    "    \n",
    "    sylDurations = spurtEndTime - spurtStartTime\n",
    "    \n",
    "    ftrs = tempOut    \n",
    "    \n",
    "    wordLabls = np.unique(wordIndication)\n",
    "    for iterWrd in range(0, len(wordLabls)):\n",
    "        inds = [i for i, x in enumerate(wordIndication) if x == wordLabls[iterWrd]] #doing argwhere(wordIndication==wordLabls[iterWrd]\n",
    "        if len(inds)>1 :\n",
    "            ftrs[-1, inds] = ftrs[-1, inds] / sum(ftrs[-1, inds])\n",
    "            ftrs[-2, inds] = ftrs[-2, inds] / sum(ftrs[-2, inds])\n",
    "    end=1\n",
    "    # print(ftrs.shape)\n",
    "    fa = ftrs\n",
    "\n",
    "    print(fa.shape)\n",
    "    print(\"fa\")\n",
    "    print(fa)\n",
    "    \n",
    "    mat = scipy.io.loadmat(stressLabelspath + file_name + '.mat')\n",
    "    lab = mat['spurtStress']\n",
    "    lab_list = lab.tolist()\n",
    "    # print(lab_list)\n",
    "\n",
    "    # print(len(lab_list))\n",
    "    \n",
    "    if (fa.shape[1] is not len(lab_list)):\n",
    "        # label_mismatch = label_mismatch+1\n",
    "#            is_looping = False\n",
    "        return None, False \n",
    "    \n",
    "    else:\n",
    "        fb,filenm = get_labels(lab_list,fa,file_name)\n",
    "        feats = fb #features ,last row:labels\n",
    "        # print(\"feats\")\n",
    "        # print(feats)\n",
    "        # print(feats.shape)\n",
    "        w=[]#polysyl_feat=[];\n",
    "        for w_l in range(len(words)):\n",
    "    #        cou = 0\n",
    "            w_st = spurtWordTimes[w_l][0]\n",
    "            w_ed = spurtWordTimes[w_l][1]\n",
    "            for s_l in range(len(w),len(spurtSyl)):\n",
    "                sy_st = spurtSylTimes[s_l][0]\n",
    "                sy_ed = spurtSylTimes[s_l][1]\n",
    "                if (sy_ed <= w_ed):\n",
    "                    w.append(w_l+1)\n",
    "#                        w.append('W'+str(w_l+1))\n",
    "    #                cou=cou+1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # print(\"length of w = \",len(w))\n",
    "        if (len(w)>np.shape(feats)[1]):\n",
    "            return None, False \n",
    "        \n",
    "        # print(w)\n",
    "        feats = np.vstack((feats,w))#features ,last row:labels, word labels\n",
    "        return feats, True\n",
    "        \n",
    "        print(feats.shape)\n",
    "        print(\"feats\")\n",
    "        print(feats)\n",
    "        \n",
    "        # AF_inform = filenm\n",
    "        # CF_feats,CF_inform = contextFeats(spurtSyl,spurtSylTimes,spurtWordTimes,vowel);  \n",
    "        # if i == 0:#411 or fileN ==412:\n",
    "        #     AF = feats\n",
    "        #     AF_info = AF_inform\n",
    "        #     CF = CF_feats\n",
    "        #     CF_info =CF_inform                \n",
    "        # else:\n",
    "        #     AF = np.hstack((AF,feats)) \n",
    "        #     AF_info = np.hstack((AF_info,AF_inform))\n",
    "        #     CF = np.hstack((CF,CF_feats))\n",
    "        #     CF_info = np.hstack((CF_info,CF_inform))\n",
    "        #     done=done+1\n",
    "     \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data array [['0.0' '0.61' 'sil']\n",
      " ['0.61' '0.79' 'ay']\n",
      " ['0.79' '0.95' 's']\n",
      " ['0.95' '1.0' 'eh']\n",
      " ['1.0' '1.05' 'd']\n",
      " ['1.05' '1.28' 'f']\n",
      " ['1.28' '1.43' 'ay']\n",
      " ['1.43' '1.55' 't']\n",
      " ['1.55' '1.59' 'sil']\n",
      " ['1.59' '1.65' 'n']\n",
      " ['1.65' '1.68' 'aa']\n",
      " ['1.68' '1.74' 't']\n",
      " ['1.74' '1.93' 's']\n",
      " ['1.93' '2.01' 'eh']\n",
      " ['2.01' '2.07' 'n']\n",
      " ['2.07' '2.21' 't']\n",
      " ['2.21' '2.39' 'er']\n",
      " ['2.39' '3.18' 'sil']]\n",
      "phones [['ay' 's' 'eh' 'd' 'f' 'ay' 't' 'n' 'aa' 't' 's' 'eh' 'n' 't' 'er']]\n",
      "phn_times [['0.61' '0.79']\n",
      " ['0.79' '0.95']\n",
      " ['0.95' '1.0']\n",
      " ['1.0' '1.05']\n",
      " ['1.05' '1.28']\n",
      " ['1.28' '1.43']\n",
      " ['1.43' '1.55']\n",
      " ['1.59' '1.65']\n",
      " ['1.65' '1.68']\n",
      " ['1.68' '1.74']\n",
      " ['1.74' '1.93']\n",
      " ['1.93' '2.01']\n",
      " ['2.01' '2.07']\n",
      " ['2.07' '2.21']\n",
      " ['2.21' '2.39']]\n",
      "vowel [['ay' 'eh' 'ay' 'aa' 'eh' 'er']]\n",
      "vowel_start_time [['0.61' '0.95' '1.28' '1.65' '1.93' '2.21']]\n",
      "vowel_end_time [['0.79' '1.0' '1.43' '1.68' '2.01' '2.39']]\n",
      "words ['i', 'said', 'fight', 'not', 'centre']\n",
      "word_syls [['aa', 'ae', 'ah', 'ay', 'ay . ah', 'ay hh', 'eh', 'ey', 'hh ae', 'hh ah', 'hh ay', 'iy', 'oy'], ['s ae', 's ae d', 's ae . d ah', 's ah d', 's eh', 's eh d', 's eh . d ah', 's eh . d ah n', 's eh dh', 's eh . dh ah', 's eh t', 's eh . t ah', 's ey', 's ey d', 's ey . d ah', 's ey z', 's ih d', 't ey d', 'z eh d'], ['f ay', 'f ay k t', 'f ay t', 'f ay . t ah', 'f ey t', 'f iy . t ah'], ['ah . n ao . t ah', 'n aa', 'n aa t', 'n ah', 'n ah t', 'n ao', 'n ao d', 'n ao m', 'n ao n', 'n ao s', 'n ao t', 'n ao . t ah', 'n ao t sh', 'n ow', 'n ow t', 'n oy'], ['s ah n . t er', 's eh . n er', 's eh n . t ah', 's eh n . t ah r', 's eh n . t ao', 's eh n . t er', 's eh n t r', 's er n . t er r', 's iy n . t er']]\n",
      "path_indices [3, 5, 2, 2, 5]\n",
      "currTestWordSyls ay s eh d f ay t n aa t s iy n . t er\n",
      "syls_word [[1. 1. 1. 1. 2.]]\n",
      "spurtSyl ['ay', 's eh d', 'f ay t', 'n aa t', 's eh n', 't er']\n",
      "spurtWordTimes [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "spurtSylTimes [[0.61 0.79]\n",
      " [0.79 1.05]\n",
      " [1.05 1.55]\n",
      " [1.59 1.74]\n",
      " [1.74 2.07]\n",
      " [2.07 2.39]]\n",
      "syls_word [[1 1 1 1 2]]\n",
      "spurtWordTimes [[0.61 0.79]\n",
      " [0.79 1.05]\n",
      " [1.05 1.55]\n",
      " [1.59 1.74]\n",
      " [1.74 2.39]]\n",
      "(19, 6)\n",
      "fa\n",
      "[[ 0.9974598   0.66342273  0.50743517  1.00058657  0.18159308  0.2730841 ]\n",
      " [ 1.00232423  0.99779225  0.96560969  0.99952492  0.59408285  0.38867789]\n",
      " [ 0.6248217   0.63516302  0.          0.72558378  0.33477178  0.30959579]\n",
      " [ 1.86982431  2.47919565  2.12542319  1.77003375  1.65635794  0.83871377]\n",
      " [ 0.68461061  0.86049064  0.76154317  0.63794367  0.58746479  0.25929922]\n",
      " [ 5.72853832  7.77967117  6.44498472  5.25420881  4.31994163  6.23717435]\n",
      " [-0.50822922 -1.62630858 -0.97779194 -0.17702401 -1.19541933 -0.83415782]\n",
      " [ 2.58876087  4.5824806   3.41755769  2.45321833  5.72757349  2.87212049]\n",
      " [ 0.42754502  0.36325561  0.27605066  0.25856405  0.12727314  0.08322151]\n",
      " [ 1.27116436  0.30514373  0.82073134  0.22179135  0.36240777  0.25964408]\n",
      " [ 1.13905806  0.3153393   0.78691035  0.19361761  0.32413961  0.23841002]\n",
      " [ 2.09167688  0.39448901  0.35347307  0.26540984  0.30211906  0.40526399]\n",
      " [ 0.76920958  0.09610523  0.10196978  0.08488884  0.08538629  0.14917517]\n",
      " [ 3.81331958  4.97701909  4.7996274   4.47198682  4.73442157  3.7253655 ]\n",
      " [-0.4759171  -0.27173098 -0.14767059  0.25523361 -0.25081837  0.59619096]\n",
      " [ 2.53736707  1.93668819  1.96901362  2.19630738  2.06440128  2.95934778]\n",
      " [ 0.48686881  0.11860349  0.22273431  0.05709403  0.07029052  0.05127337]\n",
      " [17.          4.         14.          2.          0.29166667  0.70833333]\n",
      " [17.         25.         49.         14.          0.50793651  0.49206349]]\n"
     ]
    }
   ],
   "source": [
    "ger_train_files_subset = ger_train_files[4:5]\n",
    "all_contours = []\n",
    "all_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_train_files_subset):\n",
    "  \n",
    "\n",
    "    contours, valid = feature_contour(i, file, False)\n",
    "    if valid:\n",
    "        # print(contours.shape)\n",
    "    # print(contours)\n",
    "        all_contours.extend(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pancho' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(pancho)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pancho' is not defined"
     ]
    }
   ],
   "source": [
    "print(pancho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data array [['0.0' '0.25' 'sil']\n",
      " ['0.25' '0.42' 'ay']\n",
      " ['0.42' '0.52' 's']\n",
      " ['0.52' '0.56' 'eh']\n",
      " ['0.56' '0.59' 'd']\n",
      " ['0.59' '0.68' 'hh']\n",
      " ['0.68' '0.85' 'w']\n",
      " ['0.85' '0.98' 'ay']\n",
      " ['0.98' '1.13' 't']\n",
      " ['1.13' '1.16' 'sil']\n",
      " ['1.16' '1.26' 'n']\n",
      " ['1.26' '1.3' 'aa']\n",
      " ['1.3' '1.41' 't']\n",
      " ['1.41' '1.48' 'sil']\n",
      " ['1.48' '1.55' 'b']\n",
      " ['1.55' '1.7' 'ey']\n",
      " ['1.7' '1.84' 't']\n",
      " ['1.84' '2.54' 'sil']]\n",
      "phones [['ay' 's' 'eh' 'd' 'hh' 'w' 'ay' 't' 'n' 'aa' 't' 'b' 'ey' 't']]\n",
      "phn_times [['0.25' '0.42']\n",
      " ['0.42' '0.52']\n",
      " ['0.52' '0.56']\n",
      " ['0.56' '0.59']\n",
      " ['0.59' '0.68']\n",
      " ['0.68' '0.85']\n",
      " ['0.85' '0.98']\n",
      " ['0.98' '1.13']\n",
      " ['1.16' '1.26']\n",
      " ['1.26' '1.3']\n",
      " ['1.3' '1.41']\n",
      " ['1.48' '1.55']\n",
      " ['1.55' '1.7']\n",
      " ['1.7' '1.84']]\n",
      "vowel [['ay' 'eh' 'ay' 'aa' 'ey']]\n",
      "vowel_start_time [['0.25' '0.52' '0.85' '1.26' '1.55']]\n",
      "vowel_end_time [['0.42' '0.56' '0.98' '1.3' '1.7']]\n",
      "words ['i', 'said', 'white', 'not', 'bait']\n",
      "word_syls [['aa', 'ae', 'ah', 'ay', 'ay . ah', 'ay hh', 'eh', 'ey', 'hh ae', 'hh ah', 'hh ay', 'iy', 'oy'], ['s ae', 's ae d', 's ae . d ah', 's ah d', 's eh', 's eh d', 's eh . d ah', 's eh . d ah n', 's eh dh', 's eh . dh ah', 's eh t', 's eh . t ah', 's ey', 's ey d', 's ey . d ah', 's ey z', 's ih d', 't ey d', 'z eh d'], ['hh w ay t', 'w ay', 'w ay d', 'w ay t', 'w ay . t ah'], ['ah . n ao . t ah', 'n aa', 'n aa t', 'n ah', 'n ah t', 'n ao', 'n ao d', 'n ao m', 'n ao n', 'n ao s', 'n ao t', 'n ao . t ah', 'n ao t sh', 'n ow', 'n ow t', 'n oy'], ['b ae n t', 'b ay t', 'b ey', 'b ey t', 'b ey t s']]\n",
      "path_indices [3, 5, 0, 2, 3]\n",
      "currTestWordSyls ay s eh d hh w ay t n aa t b ey t s\n",
      "syls_word [[1. 1. 1. 1. 1.]]\n",
      "spurtSyl ['ay', 's eh d', 'hh w ay t', 'n aa t', 'b ey t']\n",
      "spurtWordTimes [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "spurtSylTimes [[0.25 0.42]\n",
      " [0.42 0.59]\n",
      " [0.59 1.13]\n",
      " [1.16 1.41]\n",
      " [1.48 1.84]]\n",
      "syls_word [[1 1 1 1 1]]\n",
      "spurtWordTimes [[0.25 0.42]\n",
      " [0.42 0.59]\n",
      " [0.59 1.13]\n",
      " [1.16 1.41]\n",
      " [1.48 1.84]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# all_labels = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ger_train_files):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     contours, success \u001b[39m=\u001b[39m feature_contour(i, file, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         all_contours\u001b[39m.\u001b[39mextend(contours)\n",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m file_dir \u001b[39m=\u001b[39m ger_test_dir \u001b[39mif\u001b[39;00m test_data \u001b[39melse\u001b[39;00m ger_train_dir\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m Fs, eng_full, xx \u001b[39m=\u001b[39m vocoder_func(file_dir \u001b[39m+\u001b[39;49m wav_file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# print(\"eng_full\", eng_full)     # eng_full contains the sub-band energies of the audio file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# print(\"xx\", xx)                 # xx contains the amplitude of the waveform of the audio file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m eng_full \u001b[39m=\u001b[39m eng_full\u001b[39m.\u001b[39mconj()\u001b[39m.\u001b[39mtranspose()\n",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m fltFrame \u001b[39m=\u001b[39m butter_bandpass_filter(currFrame[\u001b[39m0\u001b[39m], fltFc[\u001b[39m0\u001b[39m][i], fltFc[\u001b[39m1\u001b[39m][i], Fs, \u001b[39m2\u001b[39m); fltFrame \u001b[39m=\u001b[39m fltFrame\u001b[39m.\u001b[39mT  \u001b[39m# this line applies the bandpass filter to the current frame with fltFc[0][i] as lowcut and fltFc[1][i] as highcut\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m rectFrame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(fltFrame[\u001b[39m0\u001b[39m:nWndw])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m lpFltFrame \u001b[39m=\u001b[39m butter_lowpass_filter(rectFrame, \u001b[39mfloat\u001b[39;49m(fltLpFc), Fs, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m currEnergy \u001b[39m=\u001b[39m lpFltFrame[nWndw\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m \u001b[39mif\u001b[39;00m currEnergy \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbutter_lowpass_filter\u001b[39m(data, lowcut, fs, order):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m     b, a \u001b[39m=\u001b[39m butter_lowpass(lowcut, fs, order\u001b[39m=\u001b[39morder)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m     y \u001b[39m=\u001b[39m lfilter(b, a, data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\signal\\_signaltools.py:2069\u001b[0m, in \u001b[0;36mlfilter\u001b[1;34m(b, a, x, axis, zi)\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2068\u001b[0m     \u001b[39mif\u001b[39;00m zi \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         \u001b[39mreturn\u001b[39;00m _sigtools\u001b[39m.\u001b[39;49m_linear_filter(b, a, x, axis)\n\u001b[0;32m   2070\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2071\u001b[0m         \u001b[39mreturn\u001b[39;00m _sigtools\u001b[39m.\u001b[39m_linear_filter(b, a, x, axis, zi)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train data\n",
    "all_contours = []\n",
    "# all_labels = []\n",
    "for i, file in enumerate(ger_train_files):\n",
    "    contours, success = feature_contour(i, file, False)\n",
    "\n",
    "    if success:\n",
    "        all_contours.extend(contours)\n",
    "        # all_labels.extend(labels)   \n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_train_files)), end=\"\\r\")\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_train_files)*100), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': all_contours})\n",
    "df.to_pickle('../saved/ger_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00%\rProcessed: 1768/1768\r"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "test_chunks = []\n",
    "# test_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_test_files):\n",
    "    chunks, success = feature_contour(i, file, True)\n",
    "    \n",
    "    if success:\n",
    "        test_chunks.extend(chunks)\n",
    "        # test_labels.extend(labels)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_test_files)*100), end=\"\\r\")\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_test_files)), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': test_chunks})\n",
    "df.to_pickle('../saved/ger_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the pkl file to csv\n",
    "df = pd.read_pickle('data.pkl')\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path,featType):#if featType = 1 ==> acoustic\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "    if featType == 1:\n",
    "     AF = data['AF']; x = AF[0:-2]; y = AF[-2]; w = AF[-1];\n",
    "    else:\n",
    "        AF = data['AF']; x1 = AF[0:-2]; x2 = data['CF']; y = AF[-2]; w = AF[-1];\n",
    "        x = np.concatenate((x1,x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a csv file into a data frame \n",
    "df = pd.read_csv('data.csv')\n",
    "# print(df.head())\n",
    "\n",
    "# convert a data frame into a numpy array\n",
    "data = df.to_numpy()\n",
    "# print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('model.pkl')\n",
    "\n",
    "path = '/content/7.jpg'\n",
    "name = 7\n",
    "img = cv2.imread(path) #bgr to rgb\n",
    "img = cv2.resize(img, (100, 100))\n",
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "features = extraction(img_hsv)\n",
    "\n",
    "features = np.array(features)\n",
    "features = features.reshape(1, -1)\n",
    "\n",
    "soc_level = loaded_model.predict(features)\n",
    "print(\"predicted soc level\", soc_level)\n",
    "print(\"actual value\", [y[name-1]])\n",
    "\n",
    "DATADIR = \"/content/drive/MyDrive/kit_2_final/kit_2\"\n",
    "\n",
    "X = [0]*(len(os.listdir(DATADIR)))\n",
    "rows = 100\n",
    "cols = 100\n",
    "prediction = []\n",
    "for img in os.listdir(DATADIR):\n",
    "    path = os.path.join(DATADIR,img)\n",
    "    # print(path)\n",
    "    name = img.split(\".\")[0]\n",
    "    name = int(name)\n",
    "    # print(name)\n",
    "    img_array = cv2.imread(path)\n",
    "    image = cv2.resize(img_array, (rows, cols))\n",
    "    img_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    features = extraction(img_hsv)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(1, -1)\n",
    "    soc_level = loaded_model.predict(features)\n",
    "    # append the soc level to the list prediction \n",
    "    prediction.append (soc_level)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# squeeze the prediction list\n",
    "prediction = np.squeeze(prediction)\n",
    "\n",
    "# list all the elements in the prediction list that are less than `0.5`\n",
    "\n",
    "list = []\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] < 0.5:\n",
    "        list.append(prediction[i])\n",
    "\n",
    "# find average of the list\n",
    "average = sum(list) / len(list)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to get the image out of the link \n",
    "# https://soilimages.s3.amazonaws.com/b7a28b21-96a9-44f0-be51-47cc98c95265.jpg\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "url = 'https://soilimages.s3.amazonaws.com/b7a28b21-96a9-44f0-be51-47cc98c95265.jpg'\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = np.array(img)\n",
    "\n",
    "\n",
    "# download the image from the link\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://soilimages.s3.amazonaws.com/b7a28b21-96a9-44f0-be51-47cc98c95265.jpg', 'image.jpg')\n",
    "\n",
    "# display the image \n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# crop the image \n",
    "crop_img = img[0:100, 0:100]\n",
    "plt.imshow(crop_img)\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('/content/DEVICE DATA3.xlsx')\n",
    "\n",
    "# load a xlsx file into a DataFrame\n",
    "df = pd.read_excel('/content/DEVICE DATA3.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   extract the entries in the third column one by one and assign it to url\n",
    "for i in range(len(new_df)):\n",
    "    url = new_df[i]\n",
    "    print(url)\n",
    "\n",
    "for i in range(len(new_df)):\n",
    "    print(new_df[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared Score:\", r2)\n",
    "\n",
    "\n",
    "# plot an roc curve for the model\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# generate a random prediction\n",
    "\n",
    "y_pred = [random.randint(0, 1) for _ in range(100)]\n",
    "y_test = [random.randint(0, 1) for _ in range(100)]\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot the roc curve\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' %roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.0])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate absolute differences between y_true and y_pred\n",
    "differences = abs(y_test - y_pred)\n",
    "\n",
    "# Convert regression problem to binary classification problem\n",
    "y_true_binary = (differences <= 0.16]).astype(int)\n",
    "\n",
    "y_ = \n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_true_binary, differences)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "y_ = np.ones(len(y_true_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign all the values in y less than 1 to 1 and all the values greater than 1 to 0\n",
    "y_[y_true_binary < 1] = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94f4693ad758032fd28f9ecc08daa4776caeeaf7af79843cea3e1525fe817927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
