{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Prominence Detection\n",
    "Speech Prominence Detection is the process of identifying the most important or prominent parts of speech in an audio signal. Prominence refers to the degree of emphasis or attention that a particular word or phrase receives in spoken language, which is often conveyed through variations in pitch, loudness, and timing. Speech Prominence Detection is an essential task in speech processing and natural language understanding, with a wide range of applications including speech recognition, sentiment analysis, and language translation. The goal of this project is to develop a machine learning model that can accurately identify the prominent parts of speech in a given audio signal. This project will involve feature extraction, model training, and evaluation, with the aim of achieving high accuracy and generalizability on a diverse range of speech datasets. The results of this project could have significant implications for improving speech recognition and understanding systems in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import struct\n",
    "import math\n",
    "from scipy.signal import butter, lfilter\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxilliary functions for feature computation\n",
    "def spectral_selection(x, n):                              #out of the 19 subband energies computed, this function selects the n energies with the highest values\n",
    "    y = x.shape\n",
    "    row = y[0]\n",
    "    col = y[1]\n",
    "    xx = []\n",
    "    for i in range(0,col,1):\n",
    "        v = x[:,i]                                           # his line selects the i-th column of x and assigns it to the variable v\n",
    "        v = np.array([v])\n",
    "        v = v.T                 \n",
    "        t = np.array(np.arange(1,row+1)).reshape(-1,1)  #This line generates a column vector t containing values from 1 to row, representing the row numbers.\n",
    "\n",
    "        v = np.hstack((v, t))                      #This line horizontally stacks v and t, resulting in a matrix where the first column contains the values of v and the second column contains the row numbers.\n",
    "        v_sort = v[v[:,0].argsort(),]                 #This line sorts v based on the values in the first column, resulting in v_sort.\n",
    "        v_sort_sel = v_sort[row-n:row, :]             #This line selects the last n rows from v_sort and assigns the result to v_sort_sel.\n",
    "        vv = v_sort_sel[v_sort_sel[:,1].argsort(),]   #This line sorts v_sort_sel based on the values in the second column, resulting in vv\n",
    "        #tt = numpy.array([vv[:,0]])                  #The subsequent code block handles the concatenation of vv[:, 0] (the first column of vv) with the previous iterations' results stored in xx\n",
    "        if i!=0:\n",
    "            if i==1:\n",
    "                pp = np.array([xx])\n",
    "                pp = pp.T\n",
    "            else:\n",
    "                pp = xx\n",
    "            pp2 = np.array([vv[:,0]])\n",
    "            pp2 = pp2.T\n",
    "            xx = np.hstack((pp, pp2))\n",
    "        else:\n",
    "            xx = np.concatenate((xx, vv[:,0]))\n",
    "    return xx                                         # xx is a matrix containing the selected energies for all the frames with size n x col\n",
    "\n",
    "\n",
    "def temp_vec_corr(x2, t_sigma):\n",
    "    from scipy.stats import norm\n",
    "    y = x2.shape\n",
    "    row = y[0]\n",
    "    col = y[1]\n",
    "    wn = norm.pdf(np.arange(1,col+1,1), (col+1)/2, t_sigma)              # pdf function with mean = (col+1)/2 and std = t_sigma as window\n",
    "    # NOTE: if we use continue to manipulate the variable x2, (the function argument), then it gets reflected back in\n",
    "    # in the parent function. (No idea why). So create a copy of x2 and work with that.\n",
    "    x3 = np.zeros((row,col))\n",
    "    for i in range(0,row,1):\n",
    "        x3[i,:] = np.multiply(x2[i,:],wn)                                # windowing the frame energies\n",
    "    s=0\n",
    "    for i in range(0,col-1,1):\n",
    "        for j in range(i+1,col,1):\n",
    "            s+= np.multiply(x3[:,i], x3[:,j])                             # computing the correlation between the consecutive frames \n",
    "    if col!=1:\n",
    "        s = np.sqrt(np.divide(s, (col-1)*col/2))\n",
    "    else:\n",
    "        s = x3                                                             \n",
    "    return s\n",
    "\n",
    "def temporal_corr(x, win, t_sigma):\n",
    "    hwin = (win-1)/2           # hwin is the half window size\n",
    "    yy = x.shape\n",
    "    row = yy[0]\n",
    "    col = yy[1]\n",
    "\n",
    "    row = int(row)\n",
    "    hwin = int(hwin)\n",
    "\n",
    "    x = np.array([np.concatenate((np.zeros((row,hwin)), x, np.zeros((row, hwin))), axis = 1)])              # zero padding the input matrix, hwin zeros on each sides of the columns\n",
    "    y = []\n",
    "    for i in range(hwin,col+hwin,1):\n",
    "        temp2 = x[0,:,i-hwin:i+hwin+1]\n",
    "        z = temp_vec_corr(temp2, t_sigma)\n",
    "        z = np.array([z]).T\n",
    "        if i==hwin:\n",
    "            y = np.concatenate((y, z[:,0]))\n",
    "        else:\n",
    "            if i==hwin+1:\n",
    "                y = np.array([y]).T\n",
    "            y = np.hstack((y, z))\n",
    "    return y\n",
    "\n",
    "def spectral_corr(x):\n",
    "    yy = x.shape\n",
    "    row = yy[0]\n",
    "    col = yy[1]\n",
    "\n",
    "    s = np.zeros((1, col))\n",
    "    for i in range(0, row-1, 1):\n",
    "        for j in range(i+1, row, 1):\n",
    "            s = s+np.multiply(x[i,:], x[j,:])\n",
    "\n",
    "    if row!=1:\n",
    "        s = np.sqrt(np.divide(s, (row*(row-1)/2)))\n",
    "    else:\n",
    "        s = x\n",
    "    return s\n",
    "\n",
    "def statFunctions_Syl(t):\n",
    "    from scipy.stats.mstats import gmean\n",
    "    if np.min(t)<0:\n",
    "        t = np.subtract(t,min(t[0]))\n",
    "        #out = []\n",
    "        #return out\n",
    "    out = np.array([np.median(t[0]), np.mean(t[0]), gmean(np.absolute(t[0])), np.max(t[0])-np.min(t[0]), np.std(t[0])])\n",
    "    out = np.array([out]).T\n",
    "    t = np.subtract(t,np.min(t[0]))\n",
    "    t = np.divide(t, np.sum(t[0]))\n",
    "    tempArr = np.array([np.arange(1,len(t[0])+1)])\n",
    "    temporalMean = np.sum(np.multiply(tempArr,t)[0])\n",
    "    temporalStd = np.sqrt(np.sum(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),2),t[0])))\n",
    "    temporalSkewness = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),3),t[0]),np.power(temporalStd,3)))\n",
    "    temporalKurthosis = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),4),t[0]),np.power(temporalStd,4)))\n",
    "    arr1 = np.array([np.array([temporalStd, temporalSkewness, temporalKurthosis])]).T\n",
    "    out = np.vstack((out,arr1))\n",
    "    return out\n",
    "\n",
    "def statFunctions_Vwl(t):\n",
    "    if np.min(t)<0:\n",
    "        t = np.subtract(t,min(t[0]))\n",
    "        #out = []\n",
    "        #eturn out\n",
    "    out = np.array([np.median(t[0]), np.mean(t[0]), np.max(t[0])-np.min(t[0]), np.std(t[0])])\n",
    "    out = np.array([out]).T\n",
    "    t = np.subtract(t,np.min(t[0]))\n",
    "    t = np.divide(t, np.sum(t[0]))\n",
    "    tempArr = np.array([np.arange(1,len(t[0])+1)])\n",
    "    temporalMean = np.sum(np.multiply(tempArr,t)[0])\n",
    "    temporalStd = np.sqrt(np.sum(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),2),t[0])))\n",
    "    temporalSkewness = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),3),t[0]),np.power(temporalStd,3)))\n",
    "    temporalKurthosis = np.sum(np.divide(np.multiply(np.power(np.subtract(np.array([np.arange(1,len(t[0])+1)]),temporalMean),4),t[0]),np.power(temporalStd,4)))\n",
    "    arr1 = np.array([np.array([temporalStd, temporalSkewness, temporalKurthosis])]).T\n",
    "    out = np.vstack((out,arr1))\n",
    "    return out\n",
    "\n",
    "def smooth(t_cor, swin, sigma):\n",
    "    from scipy.stats import norm\n",
    "    ft = norm.pdf(np.arange(1,swin+1), (swin+1)/2, sigma)\n",
    "    ft = np.array([ft])\n",
    "    t_cor = np.array([t_cor])\n",
    "    convRes = np.zeros((1, t_cor.shape[2]+ft.shape[1]-1))\n",
    "    convRes = np.convolve(t_cor[0,0,:], ft[0,:])\n",
    "    y = convRes[np.arange((swin+1)//2-1, len(convRes)-(swin-1)//2, 1)]\n",
    "    return y\n",
    "\n",
    "def get_labels(lab_list,fa,fileName):\n",
    "        L=[]; fb=fa; filenm=[];\n",
    "        \n",
    "        for num in range(0,len(lab_list)):\n",
    "            if str((lab_list[num][0].tolist())[0]) == str('P'):\n",
    "                L.append(1)\n",
    "                filenm.append(fileName)           \n",
    "            else:\n",
    "                L.append(0)\n",
    "                filenm.append(fileName)\n",
    "        fb = np.vstack((fa,L))\n",
    "#        fb = np.vstack((fb,np.asarray(filenm,object)))\n",
    "        return fb,filenm\n",
    "\n",
    "def get_labels_seq2seq(lab_list):\n",
    "        L=[];# filenm=[];\n",
    "        \n",
    "        for num in range(0,len(lab_list)):\n",
    "            if str((lab_list[num][0].tolist())[0]) == str('P'):\n",
    "                L.append(1)\n",
    "#                filenm.append(fileName)           \n",
    "            else:\n",
    "                L.append(0)\n",
    "                #filenm.append(fileName)\n",
    "        #fb = np.vstack((fa,L))\n",
    "#        fb = np.vstack((fb,np.asarray(filenm,object)))\n",
    "        return L\n",
    "    \n",
    "def vocoder_func(wavPath):\n",
    "\n",
    "    # FILTER DEFINITIONS\n",
    "\n",
    "    def butter_bandpass(lowcut, highcut, fs, order):\n",
    "        nyq = 0.5*fs\n",
    "        low = float(lowcut) / nyq\n",
    "        high = float(highcut) / nyq\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        return b, a\n",
    "\n",
    "    def butter_lowpass(lowcut, fs, order):\n",
    "        nyq = 0.5*fs\n",
    "        low = float(lowcut) / nyq\n",
    "        b ,a = butter(order, low, btype='lowpass')\n",
    "        return b, a\n",
    "\n",
    "    def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        y = lfilter(b, a, data)\n",
    "        return y\n",
    "\n",
    "    def butter_lowpass_filter(data, lowcut, fs, order):\n",
    "        b, a = butter_lowpass(lowcut, fs, order=order)\n",
    "        y = lfilter(b, a, data)\n",
    "        return y\n",
    "\n",
    "    # FUNCTION TO READ A .wav FILE MATLAB STYLE\n",
    "\n",
    "    def readWav(wavPath):\n",
    "        waveFile = wave.open(wavPath)\n",
    "        fs = waveFile.getframerate()\n",
    "        length = waveFile.getnframes()\n",
    "        data = []\n",
    "        for i in range(0, length):\n",
    "            waveData = waveFile.readframes(1)\n",
    "            data.append(struct.unpack(\"<h\", waveData))\n",
    "        waveFile.close()\n",
    "        data = np.array([data])\n",
    "        data = data.astype(float)/np.max(np.abs(data))\n",
    "        data = data[0]\n",
    "        return data, fs, length\n",
    "\n",
    "    # BUFFER FUNCTION AS DEFINED IN MATLAB\n",
    "\n",
    "    def buffer(x, n, p=0, opt=None):\n",
    "        import numpy\n",
    "        if p >= n:\n",
    "            raise ValueError('p ({}) must be less than n ({}).'.format(p,n))\n",
    "        cols = int(numpy.ceil(len(x)/float(n-p)))+1\n",
    "        if opt == 'nodelay':\n",
    "            cols += 1\n",
    "        elif opt != None:\n",
    "            raise SystemError('Only `None` (default initial condition) and '\n",
    "                              '`nodelay` (skip initial condition) have been '\n",
    "                              'implemented')\n",
    "        b = numpy.zeros((n, cols))\n",
    "        j = 0\n",
    "        for i in range(cols):\n",
    "            if i == 0 and opt == 'nodelay':\n",
    "                b[0:n,i] = x[0:n]\n",
    "                continue\n",
    "            elif i != 0 and p != 0:\n",
    "                b[:p, i] = b[-p:, i-1]\n",
    "            else:\n",
    "                b[:p, i] = 0\n",
    "            k = j + n - p\n",
    "            n_end = p+len(x[j:k])\n",
    "            b[p:n_end,i] = x[j:k,0]\n",
    "            j = k\n",
    "        return b\n",
    "\n",
    "    fltcF= np.array([240,360,480,600,720,840,1000,1150,1300,1450,1600,1800,2000,2200,2400,2700,3000,3300,3750])\n",
    "    fltBW= np.array([120,120,120,120,120,120,150,150,150,150,150,200,200,200,200,300,300,300,500])\n",
    "\n",
    "    fltFc= np.array([np.subtract(fltcF,np.divide(fltBW,2)),np.add(fltcF,np.divide(fltBW,2))])\n",
    "    fltLpFc= 50\n",
    "\n",
    "    sig, Fs, length = readWav(wavPath)\n",
    "    # print(\"sig.shape: \", sig.shape)\n",
    "    # print(\"Fs: \", Fs)\n",
    "\n",
    "    # Saving the audio in a txt file\n",
    "    xx = np.append(Fs,sig)                       # sig is the amplitude of the audio signal\n",
    "\n",
    "    nWndw = int(round(Fs*0.02))\n",
    "    # print(\"nWndw is: \", nWndw)\n",
    "\n",
    "    nOverlap = int(round(Fs*0.01))\n",
    "    # print(\"nOverlap is: \", nOverlap)\n",
    "\n",
    "    sig = 0.99*sig/max(abs(sig))                 # Normalizing the signal\n",
    "    \n",
    "    # Windowing first and filtering next\n",
    "    sigFrames= buffer(sig*32768,nWndw,nOverlap)          # sig Frames is a 2D array where each column represents an analysis frame \n",
    "    subBandEnergies= np.zeros([19,sigFrames.shape[1]])   # 2D array with 19 rows and number of columns equal to number of frames\n",
    "\n",
    "    for j in range(0,sigFrames.shape[1]): \n",
    "        currFrame = np.array([sigFrames[:,j]])                  # 1D array with the selected frame\n",
    "        for i in range(0,fltFc.shape[1]):\n",
    "            fltFrame = butter_bandpass_filter(currFrame[0], fltFc[0][i], fltFc[1][i], Fs, 2); fltFrame = fltFrame.T  # this line applies the bandpass filter to the current frame with fltFc[0][i] as lowcut and fltFc[1][i] as highcut\n",
    "            rectFrame = np.abs(fltFrame[0:nWndw])\n",
    "            lpFltFrame = butter_lowpass_filter(rectFrame, float(fltLpFc), Fs, 2)\n",
    "            currEnergy = lpFltFrame[nWndw-1]\n",
    "            if currEnergy < 1:\n",
    "                currEnergy = 0.5\n",
    "            subBandEnergies[i,j] = math.exp(2*math.log(currEnergy)/math.log(10))\n",
    "    subBandEnergies = np.concatenate((np.exp(0.5*np.ones((19,1))),subBandEnergies[:,0:-2]),axis=1).T\n",
    "\n",
    "    return Fs, subBandEnergies, xx\n",
    "\n",
    "# so you have the signal in frames. Each column is a frame and the frames and are obtained by windowing the original signal and the frames have a 50% overlap.\n",
    "# you calculate the subbandenergies frame by frame \n",
    "# for each frame you bandpass filter it through the 19 subbandenergies and take log of the last value of the filtered signal as the sub-band energy value.\n",
    "# you concatenate a 0.5 to the first column of the subbandenergies matrix and remove the last two columns of the subbandenergies matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import scipy\n",
    "from scipy.signal import medfilt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_train_dir = \"/libri-360-train_wav/wav_matched_files/\"\n",
    "# ita_train_dir = os.path.join(data_dir, \"ITA/train/\")\n",
    "\n",
    "phn_dir = \"/Harsha/FA_results/\"           # phonemes for vowels \n",
    "syl_dir = \"/Harsha/syllables/\"          # syllables \n",
    "# word_dir = \"/Harsha/words/\"             # words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Train directory does not exist\n",
      "Phoneme directory does not exist\n",
      "Syllable directory does not exist\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(ger_train_dir):\n",
    "    print(\"German Train directory does not exist\")\n",
    "\n",
    "# if not os.path.exists(ita_train_dir):\n",
    "#     print(\"Italian Train directory does not exist\")\n",
    "\n",
    "if not os.path.exists(phn_dir):\n",
    "    print(\"Phoneme directory does not exist\")\n",
    "\n",
    "if not os.path.exists(syl_dir):\n",
    "    print(\"Syllable directory does not exist\")\n",
    "\n",
    "# if not os.path.exists(word_dir):\n",
    "#     print(\"Word directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/libri-360-train_wav/wav_matched_files/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\librispeech_features.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/librispeech_features.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ger_train_files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(ger_train_dir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/libri-360-train_wav/wav_matched_files/'"
     ]
    }
   ],
   "source": [
    "ger_train_files = os.listdir(ger_train_dir)\n",
    "# ita_train_files = os.listdir(ita_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features\n",
    "twin = 5\n",
    "t_sigma = 1.4\n",
    "swin = 7\n",
    "s_sigma = 1.5\n",
    "mwin = 13\n",
    "max_threshold = 25\n",
    "\n",
    "vwlSB_num = 4\n",
    "vowelSB = [1, 2, 4, 5, 6, 7, 8, 13, 14, 15, 16, 17]\n",
    "sylSB_num = 5\n",
    "sylSB = [1, 2, 3, 4, 5, 6, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "startWordFrame_all = []\n",
    "spurtStartFrame_all = []\n",
    "spurtEndFrame_all = []\n",
    "vowelStartFrame_all = []\n",
    "vowelEndFrame_all = []\n",
    "eng_full_all = []\n",
    "spurtStress_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllables(syl_file):\n",
    "    syl_array = []\n",
    "    try:\n",
    "        fid = open(syl_file, 'r')\n",
    "        syl_array = np.loadtxt(fid, usecols=(0, 1, 2), dtype={'names': ('a', 'b', 'c'), 'formats': ('i4', 'f4', 'f4')})\n",
    "        fid.close\n",
    "    except:\n",
    "        print('File does not exist')\n",
    "        return\n",
    "\n",
    "    labels = syl_array['a']\n",
    "    spurtStartTimes = syl_array['b']\n",
    "    spurtEndTimes = syl_array['c']\n",
    "\n",
    "    return spurtStartTimes, spurtEndTimes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vowels(phn_file):\n",
    "    phn_array = []\n",
    "    try:\n",
    "        fid = open(phn_file, 'r')\n",
    "        phn_array = np.loadtxt(fid, usecols=(0, 1, 2), dtype={'names': ('a', 'b', 'c'), 'formats': ('f4', 'f4', 'S16')})\n",
    "        fid.close\n",
    "    except:\n",
    "        print('File does not exist')\n",
    "        return\n",
    "\n",
    "\n",
    "    phnStartTimes = phn_array['a']\n",
    "    phnEndTimes = phn_array['b']\n",
    "    phoneme = phn_array['c']\n",
    "    # convert labels to string\n",
    "    phoneme = [x.decode('UTF-8') for x in phoneme]\n",
    "    \n",
    "    vowelList = ['aa', 'ae', 'ah', 'ao', 'aw', 'ay', 'eh', 'er', 'ey', 'ih', 'iy', 'ow', 'oy', 'uh', 'uw']\n",
    "    vowel_start_time = []\n",
    "    vowel_end_time = []\n",
    "    vowel = []\n",
    "\n",
    "            \n",
    "    for i in range(0, len(phoneme)):\n",
    "        # convert phoneme[i] to only alphabets \n",
    "        phoneme[i] = re.sub('[^A-Za-z]+', '', phoneme[i])\n",
    "        if phoneme[i].lower() in vowelList:\n",
    "            vowel_start_time.append(phnStartTimes[i])\n",
    "            vowel_end_time.append(phnEndTimes[i])\n",
    "            vowel.append(phoneme[i].lower())\n",
    "\n",
    "    return vowel_start_time, vowel_end_time, vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(word_file):\n",
    "    word_array = []\n",
    "    try:\n",
    "        fid = open(word_file, 'r')\n",
    "        word_array = np.loadtxt(fid, usecols=(2, 4), dtype={'names': ('a', 'b'), 'formats': ('f4', 'S16')})\n",
    "        fid.close\n",
    "    except:\n",
    "        print('File does not exist')\n",
    "        return\n",
    "\n",
    "    startWordTimes = word_array['a']\n",
    "    words = word_array['b']\n",
    "    # convert words to string\n",
    "    words = [x.decode('UTF-8') for x in words]\n",
    "    return startWordTimes, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowelStartTime, startWordFrame):\n",
    "    # TCSSBC computation\n",
    "    # vowelStartTime = np.squeeze(vowelStartTime)        #NOTE you changed this, not in original code \n",
    "    startWordFrame = np.squeeze(startWordFrame)        #NOTE you changed this, not in original code \n",
    "\n",
    "    if len(sylSB) > sylSB_num:\n",
    "        eng = spectral_selection(\n",
    "            eng_full[np.subtract(sylSB, 1), :], sylSB_num)\n",
    "    else:\n",
    "        eng = eng_full[sylSB, :]               # extract only the sub-band energies that are in the sylSB list\n",
    "\n",
    "\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)             # calculate correlation spectrally and temporally \n",
    "\n",
    "    s_cor = spectral_corr(t_cor)\n",
    "\n",
    "    sylTCSSBC = smooth(s_cor, swin, s_sigma)                # smooth the correlation\n",
    "    \n",
    "    sylTCSSBC = np.array([sylTCSSBC])     \n",
    "    # sylTCSSBC = np.array(sylTCSSBC)  \n",
    "    # print(\"sylTCSSBC.shape: \", sylTCSSBC.shape)           \n",
    "\n",
    "    start_idx = np.round(spurtStartTime[0]*100).astype(int)             # get the start index of the spurt\n",
    "\n",
    "    sylTCSSBC = np.array([sylTCSSBC[0][start_idx:-1]])     \n",
    "    # sylTCSSBC = np.array(sylTCSSBC[start_idx:-1])                  # clip the TCSSBC contour from the spurt start\n",
    "  \n",
    "    sylTCSSBC = np.divide(sylTCSSBC, max(sylTCSSBC[0]))                 # normalize the TCSSBC contour\n",
    "    # sylTCSSBC = np.divide(sylTCSSBC, max(sylTCSSBC))   \n",
    "\n",
    "    # print(\"sylTCSSBC.shape: \", sylTCSSBC.shape)\n",
    "    # print(\"sylTCSSBC: \", sylTCSSBC)\n",
    "    \n",
    "    if len(vowelSB) > vwlSB_num:                                       \n",
    "        eng = spectral_selection(eng_full[np.subtract(vowelSB, 1), :], vwlSB_num)\n",
    "    else:\n",
    "        eng = eng_full[vowelSB, :]                                       # extract only the sub-band energies that are in the vowelSB list\n",
    "\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)\n",
    "    s_cor = spectral_corr(t_cor)                     \n",
    "    vwlTCSSBC = smooth(s_cor, swin, s_sigma)      \n",
    "\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC])\n",
    "    # vwlTCSSBC = np.array(vwlTCSSBC)\n",
    "    # print(\"vwlTCSSBC.shape: \", vwlTCSSBC.shape)\n",
    "\n",
    "    # Modify TCSSBC contour by clipping from the vowel start\n",
    "    start_idx = np.round(vowelStartTime[0]*100).astype(int)         # get the start index of the vowel\n",
    "    # vwlTCSSBC = np.array([vwlTCSSBC[0][start_idx:-1]])                 # clip the TCSSBC contour from the vowel start\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC[0][start_idx:-1]])   \n",
    "    # print(\"vwlTCSSBC.shape: \", vwlTCSSBC.shape)\n",
    "    # print(\"vwlTCSSBC: \", vwlTCSSBC)\n",
    "\n",
    "    vwlTCSSBC = np.divide(vwlTCSSBC, max(vwlTCSSBC[0]))                 # normalize the TCSSBC contour\n",
    "    # vwlTCSSBC = np.divide(vwlTCSSBC, max(vwlTCSSBC))   \n",
    "\n",
    "    word_duration = np.zeros((1, len(startWordFrame) - 1))\n",
    "  \n",
    "    word_Sylsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "    \n",
    "    word_Vwlsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "    \n",
    "    for j in range(0, len(startWordFrame) - 1):\n",
    "        temp_start = startWordFrame[j].astype(int)\n",
    "        temp_end = startWordFrame[j + 1].astype(int) - 1\n",
    "\n",
    "        if (temp_end >= sylTCSSBC.shape[1]):\n",
    "           \n",
    "            temp_end1 = sylTCSSBC.shape[1]-1\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end1)] = medfilt(sylTCSSBC[0, np.arange(temp_start, temp_end1)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end1] = sylTCSSBC[0, temp_end1 - 1]                          # median filteringv the TCSSBC contour\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end1)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)                                        # calculate the sum of the TCSSBC contour for the word  # need word boundaries for this \n",
    "        else:\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "                sylTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end] = sylTCSSBC[0, temp_end - 1]\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)\n",
    "            \n",
    "        if (temp_end >= vwlTCSSBC.shape[1]):\n",
    "            temp_end = vwlTCSSBC.shape[1]-1\n",
    "\n",
    "        # temp_end = np.min([temp_end,len(vwlTCSSBC)])\n",
    "        vwlTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "            vwlTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "        vwlTCSSBC[0, temp_start] = vwlTCSSBC[0, temp_start+1]\n",
    "        vwlTCSSBC[0, temp_end] = vwlTCSSBC[0, temp_end - 1]\n",
    "\n",
    "        word_duration[0, j] = temp_end - temp_start + 1                      # calculate the duration of the word in frames ## need word boundaries for this\n",
    "\n",
    "        tempArr = vwlTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "        word_Vwlsum[0, j] = tempArr.sum(axis=0)                              # calculate the sum of the TCSSBC contour for the word  # need word boundaries for this\n",
    "\n",
    "    sylTCSSBC[np.isnan(sylTCSSBC)] = 0   # Feature vector 1\n",
    "    vwlTCSSBC[np.isnan(vwlTCSSBC)] = 0   # Feature vector 2\n",
    "    return sylTCSSBC, vwlTCSSBC, word_Sylsum, word_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_contour(i, wav_file):\n",
    "    # file_name = wav_file[:-4]\n",
    "    # # print(\"file_name\", file_name)\n",
    "    # syl_file = syl_dir + file_name + \".txt\"\n",
    "    # # print(\"syl_file\", syl_file)\n",
    "    # phn_file = phn_dir + file_name + \".txt\"\n",
    "    # # print(\"phn_file\", phn_file)\n",
    "    # word_file = word_dir + file_name + \".txt\" \n",
    "    # # print(\"word_file\", word_file)\n",
    "    \n",
    "    # if not os.path.exists(phn_file):\n",
    "    #     print(\"phn file doesn't exist\")\n",
    "    #     return None, False\n",
    "    \n",
    "    # if not os.path.exists(syl_file):    \n",
    "    #     print(\"syl file doesn't exist\")\n",
    "    #     return None, False\n",
    "    \n",
    "    # if not os.path.exists(word_file):\n",
    "    #     print(\"word file doesn't exist\")\n",
    "    #     return None, False\n",
    "    \n",
    "    spurtStartTime, spurtEndTime, labels = get_syllables(\"14-208-0001_syl.txt\")\n",
    "    spurtStartTime = np.array(spurtStartTime)\n",
    "    spurtEndTime = np.array(spurtEndTime)\n",
    "    num_syls = len(spurtStartTime) \n",
    "    print(\"num_syls\", num_syls)\n",
    "    # print(spurtStartTime)\n",
    "    # print(spurtEndTime)\n",
    "    # print(labels)\n",
    "\n",
    "    vowelStartTime, vowelEndTime, vowels = get_vowels(\"14-208-0001_vwl.txt\")            \n",
    "    vowelStartTime = np.array(vowelStartTime)            #NOTE why is this not truncated to two decimal places?\n",
    "    vowelEndTime = np.array(vowelEndTime)\n",
    "    if num_syls != len(vowelStartTime):\n",
    "        print(\"Number of syllables and vowels don't match\")\n",
    "        return None, False\n",
    "\n",
    "    # print(vowelStartTime)\n",
    "    # print(vowelEndTime)\n",
    "    # print(vowels)\n",
    "\n",
    "    startWordTime, words = get_words(\"14-208-0001_word.txt\")\n",
    "    startWordTime = np.array(startWordTime)\n",
    "    # print(startWordTime)\n",
    "    # print(words)\n",
    "    \n",
    "\n",
    "\n",
    "    # Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\n",
    "    file_dir = ger_train_dir\n",
    "    Fs, eng_full, xx = vocoder_func(\"14-208-0001.wav\")\n",
    "    #  Fs, eng_full, xx = vocoder_func(file_dir + wav_file)\n",
    "\n",
    "    # print(\"eng_full\", eng_full)     # eng_full contains the sub-band energies of the audio file\n",
    "    # print(\"xx\", xx)                 # xx contains the amplitude of the waveform of the audio file\n",
    "\n",
    "    eng_full = eng_full.conj().transpose()\n",
    "    # print(\"eng_full\", eng_full)\n",
    "    print(eng_full.shape)            #NOTE fix this \n",
    "    print(Fs)\n",
    "\n",
    "    startWordFrame = np.round((np.subtract(np.array(startWordTime, dtype='float'), spurtStartTime[0].astype(float))*100))              #NOTE why is this being done? \n",
    "    # endWordFrame = np.round((np.subtract(np.array(endWordTime, dtype='float'), spurtStartTime[0].astype(float))*100) + 1)\n",
    "    startWordFrame = startWordFrame.astype(int)\n",
    "     \n",
    "    spurtStartFrame = np.round((spurtStartTime - spurtStartTime[0]) * 100)\n",
    "    spurtEndFrame = np.round((spurtEndTime - spurtStartTime[0]) * 100)\n",
    "    spurtStartFrame = spurtStartFrame.astype(int)\n",
    "    spurtEndFrame = spurtEndFrame.astype(int)\n",
    "\n",
    "    vowel_start_time = vowelStartTime.astype(float)\n",
    "    vowel_end_time = vowelEndTime.astype(float)\n",
    "\n",
    "    vowelStartFrame = np.round(vowel_start_time*100 - spurtStartTime[0]*100)   # vowelStartFrame contains the start frame of each vowel\n",
    "    vowelEndFrame = np.round(vowel_end_time*100 - spurtStartTime[0]*100)       # vowelEndFrame contains the end frame of each vowel\n",
    "    vowelStartFrame = vowelStartFrame.astype(int)\n",
    "    vowelEndFrame = vowelEndFrame.astype(int)\n",
    "  \n",
    "\n",
    "    # print(\"spurt start time = \", spurtStartTime)\n",
    "    # print(\"spurt end time = \", spurtEndTime)\n",
    "\n",
    "    # print(\"vowel start time = \", vowel_start_time)\n",
    "    # print(\"vowel end time = \", vowel_end_time)\n",
    "\n",
    "    # print(\"start word time = \", startWordTime)\n",
    "\n",
    "    # print(\"start word frame = \", startWordFrame)\n",
    "    # print(\"spurt start frame = \", spurtStartFrame)\n",
    "    # print(\"spurt end frame = \", spurtEndFrame)\n",
    "    # print(\"vowel start frame = \", vowelStartFrame)\n",
    "    # print(\"vowel end frame = \", vowelEndFrame)\n",
    "\n",
    "\n",
    "    sylTCSSBC, vwlTCSSBC, word_duration, word_Sylsum = get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowel_start_time, startWordFrame)         \n",
    "    \n",
    "    # print(\"sylTCSSBC\", sylTCSSBC)\n",
    "    # print(sylTCSSBC.shape)\n",
    "    # print(\"vwlTCSSBC\", vwlTCSSBC)\n",
    "    # print(vwlTCSSBC.shape)\n",
    "    # print(\"word_duration\", word_duration)\n",
    "    # print(\"word_Sylsum\", word_Sylsum)\n",
    "  \n",
    "    tempOut = np.array([[]])\n",
    "            \n",
    "    wordIndication = []\n",
    "\n",
    "    # Generating the features\n",
    "    for j in range(0, num_syls , 1):\n",
    "            inds = (startWordFrame <= spurtStartFrame[j]).nonzero()   # finds the word that the syllable belongs to\n",
    "            word_ind = inds[0][-1]                           # finds the index of the word that the syllable belongs to\n",
    "            wordIndication.append(word_ind)                  # stores the index of the word that the syllable belongs to\n",
    "    #       print([0, np.arange(spurtStartFrame[j], spurtEndFrame[j]-1, 1).astype(int)])\n",
    "            currFtr1SylSeg = sylTCSSBC[0, np.arange(spurtStartFrame[j], spurtEndFrame[j]-1, 1).astype(int)]  # extracts the syllable segment from the TCSSBC contour\n",
    "            currFtr1SylSeg = np.array([currFtr1SylSeg])\n",
    "            \n",
    "            temp = np.multiply(currFtr1SylSeg, len(currFtr1SylSeg[0]) / word_duration[0, word_ind])  # normalizes the syllable segment by the duration of the syllable and the duration of the word            # need word duration for this \n",
    "\n",
    "            arrResampled = np.array([librosa.resample(temp[0], Fs, Fs*float(30) / len(temp[0]), 'sinc_best')])       ##change   # resamples the syllable segment to 30 frames\n",
    "            \n",
    "            F_new = Fs*float(30) / len(temp[0])      ##change   # resampling frequency\n",
    "        \n",
    "            currSylFtrs = statFunctions_Syl(arrResampled)   # calculates the statistical features of the syllable segment\n",
    "            arr1 = np.array([np.array([np.sum(currFtr1SylSeg) / word_Sylsum[0, word_ind]])]).T     # calculates ratio of the area under the TCSSBC contour of the syllable segment and the area under the TCSSBC contour for the word\n",
    "            currSylFtrs = np.vstack((currSylFtrs, arr1))     # appends the sum of the TCSSBC contour for the word to the statistical features of the syllable segment\n",
    "        \n",
    "            if (j>= len(vowelEndFrame)):\n",
    "                break\n",
    "            if (vowelEndFrame[j] >= vwlTCSSBC.shape[1]):\n",
    "                vowelEndFrame[j] = vwlTCSSBC.shape[1]-1\n",
    "        \n",
    "            currFtr1VowelSeg = vwlTCSSBC[0, np.arange(vowelStartFrame[j], vowelEndFrame[j]-1, 1).astype(int)]   # extracts the vowel segment from the TCSSBC contour\n",
    "            currFtr1VowelSeg = np.array([currFtr1VowelSeg])\n",
    "            temp = np.multiply(currFtr1VowelSeg, len(currFtr1VowelSeg[0]) / word_duration[0, word_ind])  # normalizes the vowel segment by the duration of the syllable\n",
    "            if (len(temp[0])==0):\n",
    "                break\n",
    "                \n",
    "            arrResampled = np.array([librosa.resample(temp[0], F_new, F_new*float(20) / len(temp[0]), 'sinc_best')])     ##change  # resamples the vowel segment to 20 frames\n",
    "            currVowelFtrs = statFunctions_Vwl(arrResampled)      # calculates the statistical features of the vowel segment\n",
    "            arr1 = np.array([np.array([np.sum(currFtr1VowelSeg) / word_Sylsum[0, word_ind]])]).T        # calculates ratio of the area under the TCSSBC contour of the vowel segment and the area under the TCSSBC contour for the word\n",
    "            currVowelFtrs = np.vstack((currVowelFtrs, arr1))\n",
    "            if j == 0:\n",
    "                tempOut = np.vstack((currSylFtrs, currVowelFtrs, len(currFtr1VowelSeg[0]), len(currFtr1SylSeg[0])))    \n",
    "            else:\n",
    "                tempOut = np.hstack((tempOut, np.vstack((currSylFtrs, currVowelFtrs,len(currFtr1VowelSeg[0]), len(currFtr1SylSeg[0])))))                # tempOut columns contain the statistical features of the syllable segment and the vowel segment, the duration of the vowel segment and the duration of the syllable segment\n",
    "\n",
    "    if (len(temp[0])==0):\n",
    "            return None, False   ###\n",
    "    \n",
    "    # print(\"tempOut.shape: \", tempOut.shape)\n",
    "    # print(\"tempOut: \", tempOut)\n",
    "    \n",
    "    sylDurations = spurtEndTime - spurtStartTime\n",
    "    \n",
    "    ftrs = tempOut    \n",
    "    \n",
    "    wordLabls = np.unique(wordIndication)\n",
    "    for iterWrd in range(0, len(wordLabls)):\n",
    "        inds = [i for i, x in enumerate(wordIndication) if x == wordLabls[iterWrd]] #doing argwhere(wordIndication==wordLabls[iterWrd]\n",
    "        if len(inds)>1 :\n",
    "            ftrs[-1, inds] = ftrs[-1, inds] / sum(ftrs[-1, inds])\n",
    "            ftrs[-2, inds] = ftrs[-2, inds] / sum(ftrs[-2, inds])\n",
    "    end=1\n",
    "\n",
    "    fa = ftrs\n",
    "\n",
    "    # print(fa.shape)\n",
    "    # print(\"fa\")\n",
    "    # print(fa)\n",
    "\n",
    "    return fa, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_syls 64\n",
      "(19, 1524)\n",
      "16000\n",
      "(19, 64)\n",
      "fa\n",
      "[[1.44806515e-02 1.01911852e+00 3.58497500e-02 ... 1.20201886e+00\n",
      "  6.67410493e-01 8.05102557e-01]\n",
      " [3.98542419e-02 9.17403377e-01 5.00041690e-02 ... 9.99943380e-01\n",
      "  7.70145115e-01 2.02640149e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.44470541e-02 ... 6.21569877e-01\n",
      "  6.38310383e-01 7.64703531e-01]\n",
      " ...\n",
      " [1.51120980e-03 1.23232024e-01 9.13122061e-03 ... 5.63342574e-02\n",
      "  1.75777955e-02 1.01736300e-01]\n",
      " [1.50000000e-01 7.00000000e-01 1.50000000e-01 ... 4.00000000e+00\n",
      "  2.85714286e-01 7.14285714e-01]\n",
      " [1.71875000e-01 6.71875000e-01 1.56250000e-01 ... 2.40000000e+01\n",
      "  2.53968254e-01 7.46031746e-01]]\n"
     ]
    }
   ],
   "source": [
    "feature_contour(0, \"14-208-0001.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data array [['0.0' '0.61' 'sil']\n",
      " ['0.61' '0.79' 'ay']\n",
      " ['0.79' '0.95' 's']\n",
      " ['0.95' '1.0' 'eh']\n",
      " ['1.0' '1.05' 'd']\n",
      " ['1.05' '1.28' 'f']\n",
      " ['1.28' '1.43' 'ay']\n",
      " ['1.43' '1.55' 't']\n",
      " ['1.55' '1.59' 'sil']\n",
      " ['1.59' '1.65' 'n']\n",
      " ['1.65' '1.68' 'aa']\n",
      " ['1.68' '1.74' 't']\n",
      " ['1.74' '1.93' 's']\n",
      " ['1.93' '2.01' 'eh']\n",
      " ['2.01' '2.07' 'n']\n",
      " ['2.07' '2.21' 't']\n",
      " ['2.21' '2.39' 'er']\n",
      " ['2.39' '3.18' 'sil']]\n",
      "phones [['ay' 's' 'eh' 'd' 'f' 'ay' 't' 'n' 'aa' 't' 's' 'eh' 'n' 't' 'er']]\n",
      "phn_times [['0.61' '0.79']\n",
      " ['0.79' '0.95']\n",
      " ['0.95' '1.0']\n",
      " ['1.0' '1.05']\n",
      " ['1.05' '1.28']\n",
      " ['1.28' '1.43']\n",
      " ['1.43' '1.55']\n",
      " ['1.59' '1.65']\n",
      " ['1.65' '1.68']\n",
      " ['1.68' '1.74']\n",
      " ['1.74' '1.93']\n",
      " ['1.93' '2.01']\n",
      " ['2.01' '2.07']\n",
      " ['2.07' '2.21']\n",
      " ['2.21' '2.39']]\n",
      "vowel [['ay' 'eh' 'ay' 'aa' 'eh' 'er']]\n",
      "vowel_start_time [['0.61' '0.95' '1.28' '1.65' '1.93' '2.21']]\n",
      "vowel_end_time [['0.79' '1.0' '1.43' '1.68' '2.01' '2.39']]\n",
      "words ['i', 'said', 'fight', 'not', 'centre']\n",
      "word_syls [['aa', 'ae', 'ah', 'ay', 'ay . ah', 'ay hh', 'eh', 'ey', 'hh ae', 'hh ah', 'hh ay', 'iy', 'oy'], ['s ae', 's ae d', 's ae . d ah', 's ah d', 's eh', 's eh d', 's eh . d ah', 's eh . d ah n', 's eh dh', 's eh . dh ah', 's eh t', 's eh . t ah', 's ey', 's ey d', 's ey . d ah', 's ey z', 's ih d', 't ey d', 'z eh d'], ['f ay', 'f ay k t', 'f ay t', 'f ay . t ah', 'f ey t', 'f iy . t ah'], ['ah . n ao . t ah', 'n aa', 'n aa t', 'n ah', 'n ah t', 'n ao', 'n ao d', 'n ao m', 'n ao n', 'n ao s', 'n ao t', 'n ao . t ah', 'n ao t sh', 'n ow', 'n ow t', 'n oy'], ['s ah n . t er', 's eh . n er', 's eh n . t ah', 's eh n . t ah r', 's eh n . t ao', 's eh n . t er', 's eh n t r', 's er n . t er r', 's iy n . t er']]\n",
      "path_indices [3, 5, 2, 2, 5]\n",
      "currTestWordSyls ay s eh d f ay t n aa t s iy n . t er\n",
      "syls_word [[1. 1. 1. 1. 2.]]\n",
      "spurtSyl ['ay', 's eh d', 'f ay t', 'n aa t', 's eh n', 't er']\n",
      "spurtWordTimes [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "spurtSylTimes [[0.61 0.79]\n",
      " [0.79 1.05]\n",
      " [1.05 1.55]\n",
      " [1.59 1.74]\n",
      " [1.74 2.07]\n",
      " [2.07 2.39]]\n",
      "syls_word [[1 1 1 1 2]]\n",
      "spurtWordTimes [[0.61 0.79]\n",
      " [0.79 1.05]\n",
      " [1.05 1.55]\n",
      " [1.59 1.74]\n",
      " [1.74 2.39]]\n"
     ]
    }
   ],
   "source": [
    "ger_train_files_subset = ger_train_files[4:5]\n",
    "all_contours = []\n",
    "all_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_train_files_subset):\n",
    "  \n",
    "\n",
    "    contours, valid = feature_contour(i, file, False)\n",
    "    if valid:\n",
    "        # print(contours.shape)\n",
    "    # print(contours)\n",
    "        all_contours.extend(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data array [['0.0' '0.25' 'sil']\n",
      " ['0.25' '0.42' 'ay']\n",
      " ['0.42' '0.52' 's']\n",
      " ['0.52' '0.56' 'eh']\n",
      " ['0.56' '0.59' 'd']\n",
      " ['0.59' '0.68' 'hh']\n",
      " ['0.68' '0.85' 'w']\n",
      " ['0.85' '0.98' 'ay']\n",
      " ['0.98' '1.13' 't']\n",
      " ['1.13' '1.16' 'sil']\n",
      " ['1.16' '1.26' 'n']\n",
      " ['1.26' '1.3' 'aa']\n",
      " ['1.3' '1.41' 't']\n",
      " ['1.41' '1.48' 'sil']\n",
      " ['1.48' '1.55' 'b']\n",
      " ['1.55' '1.7' 'ey']\n",
      " ['1.7' '1.84' 't']\n",
      " ['1.84' '2.54' 'sil']]\n",
      "phones [['ay' 's' 'eh' 'd' 'hh' 'w' 'ay' 't' 'n' 'aa' 't' 'b' 'ey' 't']]\n",
      "phn_times [['0.25' '0.42']\n",
      " ['0.42' '0.52']\n",
      " ['0.52' '0.56']\n",
      " ['0.56' '0.59']\n",
      " ['0.59' '0.68']\n",
      " ['0.68' '0.85']\n",
      " ['0.85' '0.98']\n",
      " ['0.98' '1.13']\n",
      " ['1.16' '1.26']\n",
      " ['1.26' '1.3']\n",
      " ['1.3' '1.41']\n",
      " ['1.48' '1.55']\n",
      " ['1.55' '1.7']\n",
      " ['1.7' '1.84']]\n",
      "vowel [['ay' 'eh' 'ay' 'aa' 'ey']]\n",
      "vowel_start_time [['0.25' '0.52' '0.85' '1.26' '1.55']]\n",
      "vowel_end_time [['0.42' '0.56' '0.98' '1.3' '1.7']]\n",
      "words ['i', 'said', 'white', 'not', 'bait']\n",
      "word_syls [['aa', 'ae', 'ah', 'ay', 'ay . ah', 'ay hh', 'eh', 'ey', 'hh ae', 'hh ah', 'hh ay', 'iy', 'oy'], ['s ae', 's ae d', 's ae . d ah', 's ah d', 's eh', 's eh d', 's eh . d ah', 's eh . d ah n', 's eh dh', 's eh . dh ah', 's eh t', 's eh . t ah', 's ey', 's ey d', 's ey . d ah', 's ey z', 's ih d', 't ey d', 'z eh d'], ['hh w ay t', 'w ay', 'w ay d', 'w ay t', 'w ay . t ah'], ['ah . n ao . t ah', 'n aa', 'n aa t', 'n ah', 'n ah t', 'n ao', 'n ao d', 'n ao m', 'n ao n', 'n ao s', 'n ao t', 'n ao . t ah', 'n ao t sh', 'n ow', 'n ow t', 'n oy'], ['b ae n t', 'b ay t', 'b ey', 'b ey t', 'b ey t s']]\n",
      "path_indices [3, 5, 0, 2, 3]\n",
      "currTestWordSyls ay s eh d hh w ay t n aa t b ey t s\n",
      "syls_word [[1. 1. 1. 1. 1.]]\n",
      "spurtSyl ['ay', 's eh d', 'hh w ay t', 'n aa t', 'b ey t']\n",
      "spurtWordTimes [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "spurtSylTimes [[0.25 0.42]\n",
      " [0.42 0.59]\n",
      " [0.59 1.13]\n",
      " [1.16 1.41]\n",
      " [1.48 1.84]]\n",
      "syls_word [[1 1 1 1 1]]\n",
      "spurtWordTimes [[0.25 0.42]\n",
      " [0.42 0.59]\n",
      " [0.59 1.13]\n",
      " [1.16 1.41]\n",
      " [1.48 1.84]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# all_labels = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ger_train_files):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     contours, success \u001b[39m=\u001b[39m feature_contour(i, file, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         all_contours\u001b[39m.\u001b[39mextend(contours)\n",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m file_dir \u001b[39m=\u001b[39m ger_test_dir \u001b[39mif\u001b[39;00m test_data \u001b[39melse\u001b[39;00m ger_train_dir\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m Fs, eng_full, xx \u001b[39m=\u001b[39m vocoder_func(file_dir \u001b[39m+\u001b[39;49m wav_file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# print(\"eng_full\", eng_full)     # eng_full contains the sub-band energies of the audio file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# print(\"xx\", xx)                 # xx contains the amplitude of the waveform of the audio file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m eng_full \u001b[39m=\u001b[39m eng_full\u001b[39m.\u001b[39mconj()\u001b[39m.\u001b[39mtranspose()\n",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m fltFrame \u001b[39m=\u001b[39m butter_bandpass_filter(currFrame[\u001b[39m0\u001b[39m], fltFc[\u001b[39m0\u001b[39m][i], fltFc[\u001b[39m1\u001b[39m][i], Fs, \u001b[39m2\u001b[39m); fltFrame \u001b[39m=\u001b[39m fltFrame\u001b[39m.\u001b[39mT  \u001b[39m# this line applies the bandpass filter to the current frame with fltFc[0][i] as lowcut and fltFc[1][i] as highcut\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m rectFrame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(fltFrame[\u001b[39m0\u001b[39m:nWndw])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m lpFltFrame \u001b[39m=\u001b[39m butter_lowpass_filter(rectFrame, \u001b[39mfloat\u001b[39;49m(fltLpFc), Fs, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m currEnergy \u001b[39m=\u001b[39m lpFltFrame[nWndw\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m \u001b[39mif\u001b[39;00m currEnergy \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\sanke\\Documents\\Sankeerthana\\Sankeerthana\\codes\\features_modified.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbutter_lowpass_filter\u001b[39m(data, lowcut, fs, order):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m     b, a \u001b[39m=\u001b[39m butter_lowpass(lowcut, fs, order\u001b[39m=\u001b[39morder)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m     y \u001b[39m=\u001b[39m lfilter(b, a, data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/sanke/Documents/Sankeerthana/Sankeerthana/codes/features_modified.ipynb#X33sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\signal\\_signaltools.py:2069\u001b[0m, in \u001b[0;36mlfilter\u001b[1;34m(b, a, x, axis, zi)\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2068\u001b[0m     \u001b[39mif\u001b[39;00m zi \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         \u001b[39mreturn\u001b[39;00m _sigtools\u001b[39m.\u001b[39;49m_linear_filter(b, a, x, axis)\n\u001b[0;32m   2070\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2071\u001b[0m         \u001b[39mreturn\u001b[39;00m _sigtools\u001b[39m.\u001b[39m_linear_filter(b, a, x, axis, zi)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train data\n",
    "all_contours = []\n",
    "# all_labels = []\n",
    "for i, file in enumerate(ger_train_files):\n",
    "    contours, success = feature_contour(i, file)\n",
    "\n",
    "    if success:\n",
    "        all_contours.extend(contours)   #NOTE how are you storing the contours? store labels too\n",
    "        # all_labels.extend(labels)   \n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_train_files)), end=\"\\r\")\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_train_files)*100), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': all_contours})\n",
    "df.to_pickle('../saved/ger_train.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94f4693ad758032fd28f9ecc08daa4776caeeaf7af79843cea3e1525fe817927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
