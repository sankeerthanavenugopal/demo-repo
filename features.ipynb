{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Prominence Detection\n",
    "Speech Prominence Detection is the process of identifying the most important or prominent parts of speech in an audio signal. Prominence refers to the degree of emphasis or attention that a particular word or phrase receives in spoken language, which is often conveyed through variations in pitch, loudness, and timing. Speech Prominence Detection is an essential task in speech processing and natural language understanding, with a wide range of applications including speech recognition, sentiment analysis, and language translation. The goal of this project is to develop a machine learning model that can accurately identify the prominent parts of speech in a given audio signal. This project will involve feature extraction, model training, and evaluation, with the aim of achieving high accuracy and generalizability on a diverse range of speech datasets. The results of this project could have significant implications for improving speech recognition and understanding systems in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import scipy\n",
    "from scipy.signal import medfilt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myfunctions import spectral_selection,  temporal_corr, get_labels_seq2seq\n",
    "from myfunctions import spectral_corr, smooth, vocoder_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "ger_test_dir = os.path.join(data_dir, \"GER/test/\")\n",
    "ger_train_dir = os.path.join(data_dir, \"GER/train/\")\n",
    "# ita_test_dir = os.path.join(data_dir, \"ITA/test/\")\n",
    "# ita_train_dir = os.path.join(data_dir, \"ITA/train/\")\n",
    "\n",
    "phn_dir = os.path.join(data_dir, \"fisher-2000_FA_GT_ESTphnTrans_estStress/lab/txt/phn/\")\n",
    "dict_name = \"nativeEnglishDict_gt100_manoj.syl\"\n",
    "stressLabelspath = data_dir + \"FA_htkCorrectedLabWithFullAudio\" + \"/lab/mat/sylStress/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chech if the directories exist\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Data directory does not exist\")\n",
    "if not os.path.exists(ger_train_dir):\n",
    "    print(\"German Train directory does not exist\")\n",
    "# if not os.path.exists(ita_train_dir):\n",
    "#     print(\"Italian Train directory does not exist\")\n",
    "if not os.path.exists(phn_dir):\n",
    "    print(\"Phoneme directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_test_files = os.listdir(ger_test_dir)\n",
    "ger_train_files = os.listdir(ger_train_dir)\n",
    "# ita_test_files = os.listdir(ita_test_dir)\n",
    "# ita_train_files = os.listdir(ita_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features\n",
    "twin = 5\n",
    "t_sigma = 1.4\n",
    "swin = 7\n",
    "s_sigma = 1.5\n",
    "mwin = 13\n",
    "max_threshold = 25\n",
    "\n",
    "vwlSB_num = 4\n",
    "vowelSB = [1, 2, 4, 5, 6, 7, 8, 13, 14, 15, 16, 17]\n",
    "sylSB_num = 5\n",
    "sylSB = [1, 2, 3, 4, 5, 6, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "startWordFrame_all = []\n",
    "spurtStartFrame_all = []\n",
    "spurtEndFrame_all = []\n",
    "vowelStartFrame_all = []\n",
    "vowelEndFrame_all = []\n",
    "eng_full_all = []\n",
    "spurtStress_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_array(phn_file):\n",
    "    data_array = []\n",
    "    try:\n",
    "        fid = open(phn_file, 'r')\n",
    "        data_array = np.loadtxt(fid, dtype={'names': ('a', 'b', 'c'), 'formats': ('f4', 'f4', 'S16')})\n",
    "        fid.close\n",
    "    except:\n",
    "        print('File does not exist')\n",
    "        return\n",
    "\n",
    "    ghastly = []\n",
    "    for i in range(len(data_array)):\n",
    "        tuple_list = list(data_array[i])\n",
    "        tuple_list[2] = tuple_list[2].decode()\n",
    "        ghastly.append((tuple_list[0], tuple_list[1], tuple_list[2]))\n",
    "    return np.array(ghastly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phone_data(data_array):\n",
    "    phnTimes1 = [row[0] for row in data_array]\n",
    "    phnTimes1 = np.array([phnTimes1]).T\n",
    "\n",
    "    phnTimes2 = [row[1] for row in data_array]\n",
    "    phnTimes2 = np.array([phnTimes2]).T\n",
    "\n",
    "    phnTimes = np.hstack((phnTimes1, phnTimes2))\n",
    "    phones = [row[2] for row in data_array]\n",
    "    phones = np.array([phones])\n",
    "\n",
    "    # Made them lowercase since the syl dictionary is in lowercase\n",
    "    for i in range(0, len(phones[0])):\n",
    "        phones[0][i] = phones[0][i].lower()\n",
    "        \n",
    "    origPhones = phones\n",
    "    index = np.argwhere(origPhones[0] == 'sil')\n",
    "    phones = phones[phones != 'sil']\n",
    "    phones = np.array([phones])\n",
    "    phones = phones.reshape(1, -1)\n",
    "\n",
    "    phnTimes2 = np.delete(phnTimes2, index, axis=0)\n",
    "    phnTimes = np.delete(phnTimes, index, axis=0)\n",
    "    \n",
    "    return phones, phnTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vowel data\n",
    "def get_vowel_data(data_array):\n",
    "    # VOWEL LIST\n",
    "    vowelList = ['aa', 'ae', 'ah', 'ao', 'aw', 'ay', 'eh', 'er', 'ey', 'ih', 'iy', 'ow', 'oy', 'uh', 'uw']\n",
    "    vowel_start_time = []\n",
    "    vowel_end_time = []\n",
    "    vowel = []\n",
    "\n",
    "    for i in range(0, len(data_array)):\n",
    "        if data_array[i][2].lower() in vowelList:\n",
    "            vowel_start_time.append(data_array[i][0])\n",
    "            vowel_end_time.append(data_array[i][1])\n",
    "            vowel.append(data_array[i][2].lower())\n",
    "            \n",
    "    vowel_start_time = np.array([vowel_start_time])\n",
    "    vowel_end_time = np.array([vowel_end_time])\n",
    "    vowel = np.array([vowel])\n",
    "    return vowel, vowel_start_time, vowel_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_name):\n",
    "    # Define the path to the transcript file\n",
    "    trans_path = data_dir + \"ISLEtrans.txt\"\n",
    "\n",
    "    # Read the contents of the transcript file\n",
    "    with open(trans_path, 'r') as trans_file:\n",
    "        trans_contents = trans_file.read()\n",
    "\n",
    "    # Extract the lines containing the specified filename from the transcript\n",
    "    lines = [line for line in trans_contents.split('\\n') if file_name in line]\n",
    "\n",
    "    # Extract the words from the lines and clean them up\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        _, word_list = line.split(' ', 1)\n",
    "        words.extend(re.findall(r'\\b\\w+\\b', word_list))\n",
    "    words = [word.lower() for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_syls(words):\n",
    "    d = collections.defaultdict(list)\n",
    "    with open(data_dir + dict_name, 'r') as f:\n",
    "        for line in f:\n",
    "            key = line.split()[0]\n",
    "            val = line.split('=')[1].strip()\n",
    "            d[key].append(val)\n",
    "\n",
    "    word_syls = []\n",
    "    for i in range(len(words)):\n",
    "        curr_word_syls = []\n",
    "        if words[i] in d:\n",
    "            curr_word_syls = d[words[i]]\n",
    "        word_syls.append(curr_word_syls)\n",
    "    return word_syls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_indices(words, word_syls, phones):\n",
    "    newSuccessInds_all = []\n",
    "    newSuccessInds_all2 = []\n",
    "\n",
    "    prevSuccessInds_all = []\n",
    "    prevSuccessInds_all.append(0)\n",
    "\n",
    "    # I said white not bait\n",
    "    for iterWord in range(0, len(words)):\n",
    "        currWordSyls = word_syls[iterWord]   #\n",
    "        countSuccess = 1\n",
    "\n",
    "        for iterPrev in range(0, len(prevSuccessInds_all)):\n",
    "            prevWordSyls = \"\"\n",
    "            if prevSuccessInds_all[iterPrev] == 0:\n",
    "                currPrevSylInds = []\n",
    "            else:\n",
    "                currPrevSylInds = prevSuccessInds_all[iterPrev]\n",
    "                for iterPrevSyls in range(0, len(currPrevSylInds)):\n",
    "                    temp = word_syls[iterPrevSyls]\n",
    "                    prevWordSyls = prevWordSyls + \\\n",
    "                        temp[currPrevSylInds[iterPrevSyls]]+\" \"\n",
    "\n",
    "            # iterating through the syllables of the current word\n",
    "            for iterCurr in range(0, len(currWordSyls)):\n",
    "                currTestWordSyls = prevWordSyls + currWordSyls[iterCurr]\n",
    "                temp2 = currTestWordSyls.replace(' . ', ' ')\n",
    "                \n",
    "                \n",
    "                inds = [m.start() for m in re.finditer(' ', temp2)]\n",
    "                if len(inds) == 0:\n",
    "                    inds = [len(temp2)]\n",
    "\n",
    "                count = 1\n",
    "                temp = []\n",
    "\n",
    "                for iterTemp in range(len(inds)):\n",
    "                    if iterTemp == 0:\n",
    "                        temp1 = temp2[0:inds[iterTemp]]\n",
    "                        # print(temp2 + \"\\t\\t| \" + temp1)\n",
    "                    else:\n",
    "                        temp1 = temp2[inds[iterTemp-1]+1:inds[iterTemp]]\n",
    "                    if not ((np.unique(temp1) == ' ').any() or (len(temp1) == 0)):\n",
    "                        temp.append(temp1)\n",
    "                        count += 1\n",
    "                        \n",
    "                if iterTemp == len(inds) - 1 and len(inds) < len(currTestWordSyls):\n",
    "                    temp1 = temp2[inds[iterTemp]+1:len(temp2)]\n",
    "                    if not ((len(temp1) == 0) or (np.unique(temp1) == ' ').any()):\n",
    "                        temp.append(temp1)\n",
    "                        count = count+1\n",
    "\n",
    "                if iterWord + 1 == len(words):\n",
    "                    currPhones = phones[0, 0:len(phones[0])]\n",
    "                else:\n",
    "                    currPhones = phones[0][0:len(temp)]\n",
    "\n",
    "                    \n",
    "                flag = 1\n",
    "                for iterFlag in range(0, len(currPhones), 1):\n",
    "                    if len(currPhones) != len(temp):\n",
    "                        flag = 0\n",
    "                    else:\n",
    "                        if currPhones[iterFlag] != temp[iterFlag]:\n",
    "                            flag = 0\n",
    "                if flag == 1:\n",
    "                    if not currPrevSylInds == []:\n",
    "                        for i in range(0, len(currPrevSylInds)):\n",
    "                            #                            print('line 122::::::yes')\n",
    "                            newSuccessInds_all.append(currPrevSylInds[i])\n",
    "                    newSuccessInds_all.append(iterCurr)\n",
    "                    newSuccessInds_all2.append(newSuccessInds_all)\n",
    "                    newSuccessInds_all = []\n",
    "                    countSuccess = countSuccess+1\n",
    "                    \n",
    "        prevSuccessInds_all = newSuccessInds_all2\n",
    "        newSuccessInds_all2 = []\n",
    "    if len(prevSuccessInds_all) == 0:\n",
    "        return None, None\n",
    "    return prevSuccessInds_all[0], currTestWordSyls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syls_count(path_indices, currTestWordSyls, words, word_syls):\n",
    "    sylCount = 1\n",
    "    phnCount = 1\n",
    "    spurtSyl = []  # spurtSylTimes= np.zeros((len(phnTimes),2))\n",
    "\n",
    "    syls_word = np.zeros((1, len(path_indices)))\n",
    "    spurtWordTimes = np.zeros((len(path_indices), 2))\n",
    "\n",
    "    for iterPath in range(0, len(path_indices)):\n",
    "        # current word and syllables\n",
    "        currWord = words[iterPath]\n",
    "        currWordSyls = word_syls[iterPath]\n",
    "\n",
    "        currSyl = currWordSyls[path_indices[iterPath]]\n",
    "        currSyl = currSyl.replace(' . ', '.')\n",
    "        # print(currSyl)\n",
    "        inds = [m.start() for m in re.finditer('\\.', currSyl)]            # \n",
    "\n",
    "        if len(inds) == 0:\n",
    "            inds = [len(currSyl)]\n",
    "\n",
    "        count = 0\n",
    "        for iterTemp in range(0, len(inds)):\n",
    "            if iterTemp == 0:\n",
    "                temp1 = currSyl[0:inds[iterTemp]]\n",
    "            else:\n",
    "                temp1 = currSyl[inds[iterTemp-1]+1:inds[iterTemp]]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                spurtSyl.append(temp1)\n",
    "                sylCount = sylCount + 1\n",
    "                count = count + 1\n",
    "                \n",
    "        if iterTemp is len(inds)-1 and len(inds) < len(currTestWordSyls):\n",
    "            temp1 = currSyl[inds[iterTemp]+1:len(currSyl)]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                spurtSyl.append(temp1)\n",
    "                sylCount = sylCount + 1\n",
    "                count = count + 1\n",
    "        syls_word[0][iterPath] = count\n",
    "\n",
    "    return syls_word, spurtSyl, spurtWordTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "curr = 'sen'\n",
    "print(len(curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spurts(spurtSyl, currTestWordSyls, phnTimes):\n",
    "    phnCount = 1\n",
    "    spurtSylTimes = np.zeros((len(spurtSyl), 2))\n",
    "\n",
    "    for iterSyl in range(0, len(spurtSyl)):\n",
    "        temp2 = spurtSyl[iterSyl]\n",
    "        inds = [m.start() for m in re.finditer(' ', temp2)]\n",
    "        if len(inds) == 0:\n",
    "            inds = [len(temp2)]\n",
    "        count = 1\n",
    "        temp = []\n",
    "        for iterTemp in range(0, len(inds)):\n",
    "            if iterTemp == 0:\n",
    "                temp1 = temp2[0:inds[iterTemp]]\n",
    "            else:\n",
    "                temp1 = temp2[inds[iterTemp-1]+1:inds[iterTemp]]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                temp.append(temp1)\n",
    "                count = count+1\n",
    "        if iterTemp == len(inds)-1 and len(inds) < len(currTestWordSyls):\n",
    "            temp1 = temp2[inds[iterTemp]+1:len(temp2)]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                temp.append(temp1)\n",
    "                count = count+1\n",
    "\n",
    "        nPhns_syl = len(temp)\n",
    "        spurtSylTimes[iterSyl, 0] = phnTimes[phnCount-1, 0]\n",
    "        phnCount = phnCount + nPhns_syl\n",
    "        spurtSylTimes[iterSyl, 1] = phnTimes[phnCount-1-1, 1]\n",
    "    return spurtSylTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spurt_word_times(path_indices, syls_word, spurtSylTimes):\n",
    "    spurtWordTimes = np.zeros((len(path_indices), 2))\n",
    "    sylIdx = 1\n",
    "\n",
    "    for iterWordTimes in range(0, len(syls_word[0])):\n",
    "        spurtWordTimes[iterWordTimes][0] = spurtSylTimes[sylIdx-1][0]\n",
    "        sylIdx = sylIdx + syls_word[0][iterWordTimes].astype(int)\n",
    "        spurtWordTimes[iterWordTimes][1] = spurtSylTimes[sylIdx-1-1][1]\n",
    "    length_spurtWordTimes = iterWordTimes + 1\n",
    "    return spurtWordTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_boundaries(spurtWordTimes, words, spurtSylTimes):\n",
    "    # Processing word boundary file\n",
    "    # FILE READ DELETED HERE\n",
    "    a = spurtWordTimes\n",
    "    b = words\n",
    "    if (len(a) is not len(b)):\n",
    "        print(\"error\")\n",
    "    wordData = np.hstack((a, np.array([b], dtype='S32').T))\n",
    "    print(wordData)\n",
    "\n",
    "    # Extract first coloumn of wordData\n",
    "    startWordTime = [row[0] for row in wordData]\n",
    "    endWordTime = [row[1] for row in wordData]\n",
    "\n",
    "    startWordFrame = np.round((np.subtract(np.array(startWordTime, dtype='float'), spurtSylTimes[0][0].astype(float))*100))\n",
    "    endWordFrame = np.round((np.subtract(np.array(endWordTime, dtype='float'), spurtSylTimes[0][0].astype(float))*100) + 1)\n",
    "    startWordFrame = np.append(startWordFrame, endWordFrame[-1])\n",
    "\n",
    "    return startWordFrame, endWordFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowelStartTime, startWordFrame):\n",
    "    # TCSSBC computation\n",
    "    if len(sylSB) > sylSB_num:\n",
    "        eng = spectral_selection(\n",
    "            eng_full[np.subtract(sylSB, 1), :], sylSB_num)\n",
    "    else:\n",
    "        eng = eng_full[sylSB, :]\n",
    "\n",
    "    print(\"eng.shape: \", eng.shape)\n",
    "\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)\n",
    "    print(t_cor)\n",
    "    print(\"t_cor.shape: \", t_cor.shape)\n",
    "\n",
    "    s_cor = spectral_corr(t_cor)\n",
    "    # print(s_cor)\n",
    "    print(\"s_cor.shape: \", s_cor.shape)\n",
    "\n",
    "    sylTCSSBC = smooth(s_cor, swin, s_sigma)\n",
    "    # print(sylTCSSBC)\n",
    "    sylTCSSBC = np.array([sylTCSSBC])\n",
    "    print(\"sylTCSSBC.shape: \", sylTCSSBC.shape)\n",
    "\n",
    "    start_idx = np.round(spurtStartTime[0]*100).astype(int)\n",
    "    print(\"start_idx: \", start_idx)\n",
    "\n",
    "    sylTCSSBC = np.array([sylTCSSBC[0][start_idx:-1]])\n",
    "    print(\"sylTCSSBC.shape: \", sylTCSSBC.shape)\n",
    "\n",
    "    sylTCSSBC = np.divide(sylTCSSBC, max(sylTCSSBC[0]))\n",
    "\n",
    "    if len(vowelSB) > vwlSB_num:\n",
    "        eng = spectral_selection(\n",
    "            eng_full[np.subtract(vowelSB, 1), :], vwlSB_num)\n",
    "    else:\n",
    "        eng = eng_full[vowelSB, :]\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)\n",
    "    s_cor = spectral_corr(t_cor)\n",
    "    vwlTCSSBC = smooth(s_cor, swin, s_sigma)\n",
    "\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC])\n",
    "\n",
    "    # Modify TCSSBC contour by clipping from the vowel start\n",
    "    start_idx = np.round(vowelStartTime[0][0]*100).astype(int)\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC[0][start_idx:-1]])\n",
    "\n",
    "    vwlTCSSBC = np.divide(vwlTCSSBC, max(vwlTCSSBC[0]))\n",
    "    print(\"vwlTCSSBC.shape: \", vwlTCSSBC.shape)\n",
    "\n",
    "    # Compute silence statistics\n",
    "    # Preprocessing of the data\n",
    "    word_duration = np.zeros((1, len(startWordFrame) - 1))\n",
    "    word_Sylsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "    word_Vwlsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "\n",
    "    for j in range(0, len(startWordFrame) - 1):\n",
    "        temp_start = startWordFrame[j].astype(int)\n",
    "        temp_end = startWordFrame[j + 1].astype(int) - 1\n",
    "        # jhansi\n",
    "        if (temp_end >= sylTCSSBC.shape[1]):\n",
    "            temp_end1 = sylTCSSBC.shape[1]-1\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end1)] = medfilt(\n",
    "                sylTCSSBC[0, np.arange(temp_start, temp_end1)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end1] = sylTCSSBC[0, temp_end1 - 1]\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end1)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)\n",
    "        else:\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "                sylTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end] = sylTCSSBC[0, temp_end - 1]\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)\n",
    "        if (temp_end >= vwlTCSSBC.shape[1]):\n",
    "            temp_end = vwlTCSSBC.shape[1]-1\n",
    "\n",
    "        #    temp_end = np.min([temp_end,len(vwlTCSSBC)])\n",
    "        vwlTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "            vwlTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "        vwlTCSSBC[0, temp_start] = vwlTCSSBC[0, temp_start+1]\n",
    "        vwlTCSSBC[0, temp_end] = vwlTCSSBC[0, temp_end - 1]\n",
    "\n",
    "        word_duration[0, j] = temp_end - temp_start + 1\n",
    "\n",
    "        tempArr = vwlTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "        word_Vwlsum[0, j] = tempArr.sum(axis=0)\n",
    "\n",
    "    sylTCSSBC[np.isnan(sylTCSSBC)] = 0   # Feature vector 1\n",
    "    vwlTCSSBC[np.isnan(vwlTCSSBC)] = 0   # Feature vector 2\n",
    "    return sylTCSSBC[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_feature_contour(sylTCSSBC, spurtStartFrame, spurtEndFrame):\n",
    "    # Chunking the feature contour\n",
    "    sylTCSSBC_chunk = []\n",
    "    for i in range(0, len(spurtStartFrame)):\n",
    "        sylTCSSBC_chunk.append(sylTCSSBC[int(spurtStartFrame[i]):int(spurtEndFrame[i])])\n",
    "        print(i)\n",
    "        print(\"sylTCSSBC_chunk\", sylTCSSBC_chunk)\n",
    "    return np.array(sylTCSSBC_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_contour(wav_file, test_data):\n",
    "    file_name = wav_file[:-4]\n",
    "    print(\"file_name\", file_name)\n",
    "    phn_file = phn_dir + file_name + \".txt\"\n",
    "    print(\"phn_file\", phn_file)\n",
    "    mat_file = stressLabelspath + file_name + \".mat\"\n",
    "    print(\"mat_file\", mat_file)\n",
    "\n",
    "    if not os.path.exists(phn_file):\n",
    "        print(\"phn file doesn't exist\")\n",
    "        return None, None, False\n",
    "    \n",
    "    if not os.path.exists(mat_file):\n",
    "        print(\"mat file doesn't exist\")\n",
    "        return None, None, False\n",
    "\n",
    "    data_array = get_data_array(phn_file)\n",
    "    print(\"data array\", data_array)\n",
    "    phones, phn_times = get_phone_data(data_array)\n",
    "    print(\"phones\", phones)\n",
    "    print(\"phn_times\", phn_times)\n",
    "    vowel, vowel_start_time, vowel_end_time = get_vowel_data(data_array)\n",
    "    print(\"vowel\", vowel)\n",
    "    print(\"vowel_start_time\", vowel_start_time)\n",
    "    print(\"vowel_end_time\", vowel_end_time)\n",
    "    words = get_words(file_name)\n",
    "    print(\"words\", words)\n",
    "    word_syls = get_word_syls(words)\n",
    "    print(\"word_syls\", word_syls)\n",
    "    path_indices, currTestWordSyls = get_path_indices(words, word_syls, phones)    \n",
    "    print(\"path_indices\", path_indices) \n",
    "    print(\"currTestWordSyls\", currTestWordSyls)\n",
    "\n",
    "    if path_indices == None:\n",
    "        return None, None, False\n",
    "\n",
    "    syls_word, spurtSyl, spurtWordTimes = get_syls_count(path_indices, currTestWordSyls, words, word_syls)\n",
    "    print(\"syls_word\", syls_word)\n",
    "    print(\"spurtSyl\", spurtSyl)\n",
    "    print(\"spurtWordTimes\", spurtWordTimes)\n",
    "\n",
    "    spurtSylTimes = get_spurts(spurtSyl, currTestWordSyls, phn_times)\n",
    "    print(\"spurtSylTimes\", spurtSylTimes)\n",
    "\n",
    "    syls_word = syls_word.astype('i')\n",
    "    print(\"syls_word\", syls_word)\n",
    "\n",
    "    spurtWordTimes = get_spurt_word_times(path_indices, syls_word, spurtSylTimes)\n",
    "    print(\"spurtWordTimes\", spurtWordTimes)\n",
    "\n",
    "    # Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\n",
    "    file_dir = ger_test_dir if test_data else ger_train_dir\n",
    "    eng_full, xx = vocoder_func(file_dir + wav_file)\n",
    "    # print(\"eng_full\", eng_full)     # eng_full contains the sub-band energies of the audio file\n",
    "    # print(\"xx\", xx)                 # xx contains the amplitude of the waveform of the audio file\n",
    "    eng_full = eng_full.conj().transpose()\n",
    "    # print(\"eng_full\", eng_full)\n",
    "    print(eng_full.shape)\n",
    "    print(xx.shape)   \n",
    "\n",
    "    startWordFrame, endWordFrame = process_word_boundaries(spurtWordTimes, words, spurtSylTimes)\n",
    "    print(\"startWordFrame\", startWordFrame)                         # startWordFrame contains the start frame of each word\n",
    "    print(\"endWordFrame\", endWordFrame)                             # endWordFrame contains the end frame of each word\n",
    "    \n",
    "    # Processing of stress and syllable boundary file\n",
    "    spurtSylTime = spurtSylTimes\n",
    "    spurtStartTime = spurtSylTime[:, 0]\n",
    "    spurtEndTime = spurtSylTime[:, 1]\n",
    "    spurtStartFrame = np.round((spurtStartTime - spurtStartTime[0]) * 100)\n",
    "    print(\"spurtStartFrame\", spurtStartFrame)\n",
    "\n",
    "    spurtEndFrame = np.round((spurtEndTime - spurtStartTime[0]) * 100)\n",
    "    print(\"spurtEndFrame\", spurtEndFrame)\n",
    "\n",
    "    # Processing of Vowel boundary file\n",
    "    vowel_start_time = vowel_start_time.astype(float)\n",
    "    vowel_end_time = vowel_end_time.astype(float)\n",
    "\n",
    "    vowelStartFrame = np.round(vowel_start_time*100 - spurtStartTime[0]*100)   # vowelStartFrame contains the start frame of each vowel\n",
    "    vowelEndFrame = np.round(vowel_end_time*100 - spurtStartTime[0]*100)       # vowelEndFrame contains the end frame of each vowel\n",
    "    print(\"vowelStartFrame\", vowelStartFrame)\n",
    "    print(\"vowelEndFrame\", vowelEndFrame)\n",
    "\n",
    "    # print(\"words = \", words)\n",
    "    # print(\"spurt start time = \", spurtStartTime, len(spurtStartTime))\n",
    "    # print(\"vowel start time = \", vowel_start_time, len(vowel_start_time[0]))\n",
    "    # print(\"start word frame = \", startWordFrame, len(startWordFrame))\n",
    "    # print(\"spurt start frame = \", spurtStartFrame, len(spurtStartFrame))\n",
    "    # print(\"spurt end frame = \", spurtEndFrame, len(spurtEndFrame))\n",
    "\n",
    "    polym = []\n",
    "    poly = []\n",
    "    for n in syls_word[0]:\n",
    "        if n == 1:\n",
    "            polym.append(False)\n",
    "            poly.append(False)\n",
    "        else:\n",
    "            polym.extend([True] * n)\n",
    "            poly.append(True)\n",
    "    num_poly = np.sum(polym)\n",
    "    print(\"num_poly\", num_poly)\n",
    "    if num_poly == 0:\n",
    "        return None, None, False\n",
    "    \n",
    "    print(poly)\n",
    "    print(polym)\n",
    "\n",
    "    # eliminate the monosyllabic words using the poly list\n",
    "    syls_word = syls_word[0][poly]\n",
    "    print(\"syls_word\", syls_word)\n",
    "\n",
    "    spurtStartTime = spurtStartTime[polym]\n",
    "    print(\"spurtStartTime\", spurtStartTime)\n",
    "\n",
    "    vowel_start_time = np.array([vowel_start_time[0][polym]])\n",
    "    print(\"vowel_start_time\", vowel_start_time)\n",
    "\n",
    "    startWordFrame = startWordFrame[poly.append(True)]\n",
    "    print(\"startWordFrame\", startWordFrame)\n",
    "\n",
    "    spurtStartFrame = spurtStartFrame[polym]\n",
    "    print(\"spurtStartFrame\", spurtStartFrame)\n",
    "\n",
    "    spurtEndFrame = spurtEndFrame[polym]\n",
    "    print(\"spurtEndFrame\", spurtEndFrame)\n",
    "\n",
    "    # print()\n",
    "    # print(\"words = \", words)\n",
    "    # print(\"spurt start time = \", spurtStartTime, len(spurtStartTime))\n",
    "    # print(\"vowel start time = \", vowel_start_time, len(vowel_start_time))\n",
    "    # print(\"start word frame = \", startWordFrame, len(startWordFrame))\n",
    "    # print(\"spurt start frame = \", spurtStartFrame, len(spurtStartFrame))\n",
    "    # print(\"spurt end frame = \", spurtEndFrame, len(spurtEndFrame))\n",
    "\n",
    "    sylTCSSBC = get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowel_start_time, startWordFrame)\n",
    "    # print(\"sylTCSSBC\", sylTCSSBC.shape)\n",
    "\n",
    "    sylTCSSBC_chunk = chunk_feature_contour(sylTCSSBC, spurtStartFrame, spurtEndFrame)\n",
    "    print(\"sylTCSSBC_chunk\", sylTCSSBC_chunk)\n",
    "    print(\"sylTCSSBC_chunk\", sylTCSSBC_chunk.shape)\n",
    "\n",
    "    # extract label\n",
    "    mat = scipy.io.loadmat(stressLabelspath + file_name + '.mat')\n",
    "    lab = mat['spurtStress']\n",
    "    lab_list = lab.tolist()\n",
    "    labels = get_labels_seq2seq(lab_list)  # Labels\n",
    "    if len(labels) != len(polym):\n",
    "        return None, None, False\n",
    "    \n",
    "    labels = [labels[i] for i in range(len(labels)) if polym[i]]\n",
    "\n",
    "    if len(sylTCSSBC_chunk) != len(labels):\n",
    "        return None, None, False\n",
    "\n",
    "    return sylTCSSBC_chunk, labels, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name ISLE_SESS0006_BLOCKD01_05_sprt1\n",
      "phn_file ../data/fisher-2000_FA_GT_ESTphnTrans_estStress/lab/txt/phn/ISLE_SESS0006_BLOCKD01_05_sprt1.txt\n",
      "mat_file ../data/FA_htkCorrectedLabWithFullAudio/lab/mat/sylStress/ISLE_SESS0006_BLOCKD01_05_sprt1.mat\n",
      "data array [['0.0' '0.61' 'sil']\n",
      " ['0.61' '0.79' 'ay']\n",
      " ['0.79' '0.95' 's']\n",
      " ['0.95' '1.0' 'eh']\n",
      " ['1.0' '1.05' 'd']\n",
      " ['1.05' '1.28' 'f']\n",
      " ['1.28' '1.43' 'ay']\n",
      " ['1.43' '1.55' 't']\n",
      " ['1.55' '1.59' 'sil']\n",
      " ['1.59' '1.65' 'n']\n",
      " ['1.65' '1.68' 'aa']\n",
      " ['1.68' '1.74' 't']\n",
      " ['1.74' '1.93' 's']\n",
      " ['1.93' '2.01' 'eh']\n",
      " ['2.01' '2.07' 'n']\n",
      " ['2.07' '2.21' 't']\n",
      " ['2.21' '2.39' 'er']\n",
      " ['2.39' '3.18' 'sil']]\n",
      "phones [['ay' 's' 'eh' 'd' 'f' 'ay' 't' 'n' 'aa' 't' 's' 'eh' 'n' 't' 'er']]\n",
      "phn_times [['0.61' '0.79']\n",
      " ['0.79' '0.95']\n",
      " ['0.95' '1.0']\n",
      " ['1.0' '1.05']\n",
      " ['1.05' '1.28']\n",
      " ['1.28' '1.43']\n",
      " ['1.43' '1.55']\n",
      " ['1.59' '1.65']\n",
      " ['1.65' '1.68']\n",
      " ['1.68' '1.74']\n",
      " ['1.74' '1.93']\n",
      " ['1.93' '2.01']\n",
      " ['2.01' '2.07']\n",
      " ['2.07' '2.21']\n",
      " ['2.21' '2.39']]\n",
      "vowel [['ay' 'eh' 'ay' 'aa' 'eh' 'er']]\n",
      "vowel_start_time [['0.61' '0.95' '1.28' '1.65' '1.93' '2.21']]\n",
      "vowel_end_time [['0.79' '1.0' '1.43' '1.68' '2.01' '2.39']]\n",
      "words ['i', 'said', 'fight', 'not', 'centre']\n",
      "word_syls [['aa', 'ae', 'ah', 'ay', 'ay . ah', 'ay hh', 'eh', 'ey', 'hh ae', 'hh ah', 'hh ay', 'iy', 'oy'], ['s ae', 's ae d', 's ae . d ah', 's ah d', 's eh', 's eh d', 's eh . d ah', 's eh . d ah n', 's eh dh', 's eh . dh ah', 's eh t', 's eh . t ah', 's ey', 's ey d', 's ey . d ah', 's ey z', 's ih d', 't ey d', 'z eh d'], ['f ay', 'f ay k t', 'f ay t', 'f ay . t ah', 'f ey t', 'f iy . t ah'], ['ah . n ao . t ah', 'n aa', 'n aa t', 'n ah', 'n ah t', 'n ao', 'n ao d', 'n ao m', 'n ao n', 'n ao s', 'n ao t', 'n ao . t ah', 'n ao t sh', 'n ow', 'n ow t', 'n oy'], ['s ah n . t er', 's eh . n er', 's eh n . t ah', 's eh n . t ah r', 's eh n . t ao', 's eh n . t er', 's eh n t r', 's er n . t er r', 's iy n . t er']]\n",
      "path_indices [3, 5, 2, 2, 5]\n",
      "currTestWordSyls ay s eh d f ay t n aa t s iy n . t er\n",
      "syls_word [[1. 1. 1. 1. 2.]]\n",
      "spurtSyl ['ay', 's eh d', 'f ay t', 'n aa t', 's eh n', 't er']\n",
      "spurtWordTimes [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "spurtSylTimes [[0.61 0.79]\n",
      " [0.79 1.05]\n",
      " [1.05 1.55]\n",
      " [1.59 1.74]\n",
      " [1.74 2.07]\n",
      " [2.07 2.39]]\n",
      "syls_word [[1 1 1 1 2]]\n",
      "spurtWordTimes [[0.61 0.79]\n",
      " [0.79 1.05]\n",
      " [1.05 1.55]\n",
      " [1.59 1.74]\n",
      " [1.74 2.39]]\n",
      "sig.shape:  (51200, 1)\n",
      "nWndw is:  320\n",
      "nOverlap is:  160\n"
     ]
    }
   ],
   "source": [
    "ger_train_files_subset = ger_train_files[4:5]\n",
    "all_contours = []\n",
    "all_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_train_files_subset):\n",
    "  \n",
    "    contours, labels, valid = feature_contour(file, False)\n",
    " \n",
    "    if not valid: continue\n",
    "\n",
    "    all_contours.extend(contours)\n",
    "    all_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_contours)\n",
    "all_contours = np.array(all_contours)\n",
    "print(all_contours.shape)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracted phones for the corresponding aud\n",
    "removed the silence from the phones\n",
    "extracted vowels from the phones\n",
    "extracted the words from the aud\n",
    "extracted the different types of phones from the words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "all_chunks = []\n",
    "all_labels = []\n",
    "for i, file in enumerate(ger_train_files):\n",
    "    chunks, labels, success = feature_contour(file, False)\n",
    "\n",
    "    if success:\n",
    "        all_chunks.extend(chunks)\n",
    "        all_labels.extend(labels)   \n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_train_files)), end=\"\\r\")\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_train_files)*100), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = np.array(all_chunks) \n",
    "\n",
    "print(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': all_chunks, 'labels': all_labels})\n",
    "df.to_pickle('../saved/ger_train_onlypoly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_chunks = []\n",
    "test_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_test_files):\n",
    "    chunks, labels, success = feature_contour(file, True)\n",
    "    \n",
    "    if success:\n",
    "        test_chunks.extend(chunks)\n",
    "        test_labels.extend(labels)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_test_files)*100), end=\"\\r\")\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_test_files)), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': test_chunks, 'labels': test_labels})\n",
    "df.to_pickle('../saved/ger_test_onlypoly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_chunks))\n",
    "print(len(test_labels))\n",
    "print(len(all_chunks))\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe\n",
    "df = pd.DataFrame({'contour': all_chunks, 'labels': all_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df.to_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe\n",
    "df2 = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'contour': test_chunks})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the pkl file to csv\n",
    "df = pd.read_pickle('data.pkl')\n",
    "df.to_csv('data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltcF= np.array([240,360,480,600,720,840,1000,1150,1300,1450,1600,1800,2000,2200,2400,2700,3000,3300,3750])\n",
    "fltBW= np.array([120,120,120,120,120,120,150,150,150,150,150,200,200,200,200,300,300,300,500])\n",
    "\n",
    "fltFc= np.array([np.subtract(fltcF,np.divide(fltBW,2)),np.add(fltcF,np.divide(fltBW,2))])\n",
    "fltLpFc= 50\n",
    "print(fltFc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94f4693ad758032fd28f9ecc08daa4776caeeaf7af79843cea3e1525fe817927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
